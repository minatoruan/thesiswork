{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "",
  "signature": "sha256:defe8d7b855d49f6edecd61dc31f6b81a9a0766e3fdaa89a4e08386339bea90b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Common.py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile common.py\n",
      "import os\n",
      "import pickle\n",
      "import csv\n",
      "import numpy as np\n",
      "import datetime as dt\n",
      "import math\n",
      "import re\n",
      "from collections import namedtuple\n",
      "from pytz import timezone\n",
      "\n",
      "def date_time_utc_converter(isoformat):\n",
      "    if isoformat == '': return np.NaN\n",
      "    return timezone('US/Pacific').localize(dt.datetime.strptime(isoformat, '%m/%d/%y %H:%M')).astimezone(timezone('UTC')).replace(tzinfo=None)\n",
      "\n",
      "def check_2014_data(signal):\n",
      "    return '|' in signal\n",
      "\n",
      "def convert_2014_smag(arr):\n",
      "    return arr * np.sqrt(3)\n",
      "\n",
      "def convert_2014_angs(arr):\n",
      "    return np.rad2deg(arr)\n",
      "\n",
      "def compute_delta_v(datapoint, steady_states):\n",
      "    return ((sum((1 - (datapoint * 1.0 / steady_states)) ** 2))**0.5)/3.0\n",
      "\n",
      "def export_to_csv(filename, dict_data, key_headers, columns=None, desc=False):\n",
      "    # export to csv\n",
      "    with open(filename, 'w') as csvfile:\n",
      "        writer = csv.DictWriter(csvfile, fieldnames=key_headers)\n",
      "        writer.writeheader()\n",
      "        for _id in sorted(dict_data, reverse=desc):\n",
      "            data_row = {}\n",
      "            for fn in key_headers:\n",
      "                if not columns is None and fn in columns:\n",
      "                    data_row[fn] = columns[fn](_id, dict_data[_id])\n",
      "                else: \n",
      "                    data_row[fn] = dict_data[_id].get(fn, '')\n",
      "            writer.writerow(data_row)\n",
      "\n",
      "def date_time_converter(isoformat):\n",
      "    if isoformat == '': return np.NaN\n",
      "    return dt.datetime.strptime(isoformat, '%y-%m-%d %H:%M:%S.%f')\n",
      "\n",
      "def chunk_date_range(cycle, before_seconds, after_seconds):\n",
      "    starting_date = cycle - dt.timedelta(0, before_seconds, 0)\n",
      "    return [starting_date + dt.timedelta(0, i*60, 0) for i in range(1+(before_seconds + after_seconds)/60)]\n",
      "\n",
      "def get_precise_cycle_fault_signature():\n",
      "    return 'txt/precise_cycle_fault_signature.pickle'\n",
      "\n",
      "def get_precise_cycle_fault_signature_20160101():\n",
      "    return 'txt/precise_cycle_fault_signature_20160101.pickle'\n",
      "\n",
      "#def get_fn_sag_mode_find_faulted_bus():\n",
      "#    return 'txt/sag_vote_selector_events_find_faulted_bus_v1a.pickle'\n",
      "\n",
      "def get_fn_normal_minutes():\n",
      "    return 'txt/normal_array_4.pickle', 'txt/directed_normal_array_4.pickle'\n",
      "\n",
      "def get_fn_normal_minutes_20160101():\n",
      "    return 'txt/normal_array_5.pickle', ''\n",
      "\n",
      "\"\"\"\n",
      "PROBLEM_FAULTS = [(10,0), # There is no data for this fault\n",
      "                  (7,0),  # Duplicate spike of (7,1) & (7,0)'s signal is offline\n",
      "                  (16,1), # Duplicate spike of (16,0)\n",
      "                  (29,1),(29,2), # Duplicates of (29,0) spike\n",
      "                  (51,1),(51,2), # Duplicates of (51,0) spike\n",
      "                  (52,1),(52,2), # Duplicate of (52,0) spike\n",
      "                  (60,1), # Duplicate of (60,0) spike\n",
      "                  (100,0), # \"Exact\" time is incorrect, no data for this fault,\n",
      "                  (1,0), (6,0), (9,0), (25,0), (33,0), (60,0), (60,1), (92,0), (98,0), (100,0), (3,0)\n",
      "                  ]\n",
      "\"\"\"\n",
      "def savepickle(fn, obj):\n",
      "    with open(fn, 'wb') as fin:\n",
      "        pickle.dump(obj, fin)\n",
      "\n",
      "def loadpickle(events_fn, predicate = None):\n",
      "    with open(events_fn, 'rb') as fin:\n",
      "        if (not predicate): return pickle.load(fin)\n",
      "        return select(pickle.load(fin), predicate)\n",
      "\n",
      "def get_generic_class(classfication):\n",
      "    if (classfication in ['AG', 'BG', 'CG']): return 'SLG'\n",
      "    if (classfication in ['AB', 'BC', 'AC']): return 'LtL'\n",
      "    if (classfication in ['ABC']): return '3P'\n",
      "    return 'noFault'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting common.py\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Feature"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile feature.py\n",
      "import cmath\n",
      "from math import ceil\n",
      "from numpy import rad2deg, deg2rad, angle\n",
      "import numpy as np\n",
      "\n",
      "counterlockwise = lambda x: x if x > 0 else 360 - abs(x)\n",
      "def relativeAngle(x, y):\n",
      "    rel = (abs(counterlockwise(x) - counterlockwise(y)))\n",
      "    rel = rel if rel < 180 else 360 - rel\n",
      "    return rel\n",
      "\n",
      "def getPhaseAngleBySite(gen_signal, phased_angles_mapper):\n",
      "    angle_signal = gen_signal.replace('Voltage Mag', 'Voltage Ang')    \n",
      "    return phased_angles_mapper[angle_signal]\n",
      "\n",
      "def pos_seq(mag1, ang1, mag2, ang2, mag3, ang3):\n",
      "    e1 = cmath.rect(mag1, np.deg2rad(ang1))\n",
      "    e2 = cmath.rect(mag2, np.deg2rad(ang2))\n",
      "    e3 = cmath.rect(mag3, np.deg2rad(ang3))\n",
      "    a = cmath.rect(1.0, np.deg2rad(120))\n",
      "\n",
      "    V1 = (e1 + a*e2 + a**2*e3)\n",
      "    return round(abs(V1)/3,2), round(np.rad2deg(np.angle(V1)))\n",
      "\n",
      "def neg_seq(mag1, ang1, mag2, ang2, mag3, ang3):\n",
      "    e1 = cmath.rect(mag1, np.deg2rad(ang1))\n",
      "    e2 = cmath.rect(mag2, np.deg2rad(ang2))\n",
      "    e3 = cmath.rect(mag3, np.deg2rad(ang3))\n",
      "    a = cmath.rect(1.0, np.deg2rad(120))\n",
      "    \n",
      "    V2 = (e1 + a**2*e2 + a*e3)\n",
      "    return round(abs(V2)/3,2), round(np.rad2deg(np.angle(V2)))\n",
      "\n",
      "def zero_seq(mag1, ang1, mag2, ang2, mag3, ang3):\n",
      "    e1 = cmath.rect(mag1, np.deg2rad(ang1))\n",
      "    e2 = cmath.rect(mag2, np.deg2rad(ang2))\n",
      "    e3 = cmath.rect(mag3, np.deg2rad(ang3))\n",
      "    a = cmath.rect(1.0, np.deg2rad(120))\n",
      "    \n",
      "    V0 = (e1 + e2 + e3)\n",
      "    return round(abs(V0)/3,2), round(np.rad2deg(np.angle(V0)))\n",
      "\n",
      "def a_seq(mag1, ang1, mag2, ang2, mag3, ang3):\n",
      "    v0, a0 = zero_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    v1, a1 = pos_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    v2, a2 = neg_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    \n",
      "    e0 = cmath.rect(v0, np.deg2rad(a0))\n",
      "    e1 = cmath.rect(v1, np.deg2rad(a1))\n",
      "    e2 = cmath.rect(v2, np.deg2rad(a2))\n",
      "    a = cmath.rect(1.0, np.deg2rad(120))\n",
      "    \n",
      "    V0 = (e0 + e1 + e2)\n",
      "    return round(abs(V0),2), round(np.rad2deg(np.angle(V0)))\n",
      "\n",
      "def b_seq(mag1, ang1, mag2, ang2, mag3, ang3):\n",
      "    v0, a0 = zero_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    v1, a1 = pos_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    v2, a2 = neg_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    \n",
      "    e0 = cmath.rect(v0, np.deg2rad(a0))\n",
      "    e1 = cmath.rect(v1, np.deg2rad(a1))\n",
      "    e2 = cmath.rect(v2, np.deg2rad(a2))\n",
      "    a = cmath.rect(1.0, np.deg2rad(120))\n",
      "    \n",
      "    V1 = (e0 + a**2*e1 + a*e2)\n",
      "    return round(abs(V1),2), round(np.rad2deg(np.angle(V1)))\n",
      "\n",
      "def c_seq(mag1, ang1, mag2, ang2, mag3, ang3):\n",
      "    v0, a0 = zero_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    v1, a1 = pos_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    v2, a2 = neg_seq(mag1, ang1, mag2, ang2, mag3, ang3)\n",
      "    \n",
      "    e0 = cmath.rect(v0, np.deg2rad(a0))\n",
      "    e1 = cmath.rect(v1, np.deg2rad(a1))\n",
      "    e2 = cmath.rect(v2, np.deg2rad(a2))\n",
      "    a = cmath.rect(1.0, np.deg2rad(120))\n",
      "    \n",
      "    V2 = (e0 + a*e1 + a**2*e2)\n",
      "    return round(abs(V2),2), round(np.rad2deg(np.angle(V2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting feature.py\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Arff manipulation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile arff.py\n",
      "from sklearn.datasets import load_svmlight_file\n",
      "from collections import namedtuple\n",
      "from operator import itemgetter, attrgetter\n",
      "from bpati import signals\n",
      "from numpy import array, isnan, isinf, nan, append, delete, mean, min, average\n",
      "from libs import feature, common\n",
      "from scipy import stats\n",
      "import random\n",
      "import numpy as np\n",
      "\n",
      "Field = namedtuple('Field', 'description expr')\n",
      "weka_java = 'libs/weka.jar'\n",
      "\n",
      "defaultfields = [\n",
      "            Field('faultId numeric', lambda event: event.eventid[0]),\n",
      "            Field('subfaultId numeric', lambda event: event.eventid[1]),\n",
      "            Field('isfaultedbus numeric', lambda event: 1 if event.isfaultedbus else 0),\n",
      "            Field('signal numeric', lambda event: signals.SIGNALS_2013.index(event.signal)),\n",
      "            Field('v_sag_1 numeric', lambda event: event.get_vsags()[0]),\n",
      "            Field('v_sag_2 numeric', lambda event: event.get_vsags()[1]),\n",
      "            Field('v_sag_3 numeric', lambda event: event.get_vsags()[2]),\n",
      "            Field('avg_sag_1 numeric', lambda event: event.get_avg_sags()[0]),\n",
      "            Field('avg_sag_2 numeric', lambda event: event.get_avg_sags()[1]),\n",
      "            Field('avg_sag_3 numeric', lambda event: event.get_avg_sags()[2]),\n",
      "            Field('delta_sag_1 numeric', lambda event: event.get_delta_sags()[0]),\n",
      "            Field('delta_v numeric', lambda event: event.dv()),\n",
      "            Field('n_cycle_from_ss numeric', lambda event: event.distance_from_ss),\n",
      "            Field('std_ss_1 numeric', lambda event: event.std_mags[2]),\n",
      "            Field('a_change_1 numeric', lambda event: event.get_a_changes()[0]),\n",
      "            Field('a_change_2 numeric', lambda event: event.get_a_changes()[1]),\n",
      "            Field('a_change_3 numeric', lambda event: event.get_a_changes()[2]),\n",
      "            Field('rel_phase_2_1 numeric', lambda event: event.get_relative_phase_2_1()),\n",
      "            Field('rel_phase_3_1 numeric', lambda event: event.get_relative_phase_3_1()),\n",
      "            Field('rel_phase_3_2 numeric', lambda event: event.get_relative_phase_3_2()),\n",
      "            Field('sag_neg_seq_1 numeric', lambda event: event.get_neg_seq()[0]/mean(event.ss_mags)),\n",
      "            Field('sag_neg_seq_angle_1 numeric', lambda event: event.get_neg_seq()[1]/120),\n",
      "            Field('sag_pos_seq_1 numeric', lambda event: event.get_pos_seq()[0]/mean(event.ss_mags)),\n",
      "            Field('sag_pos_seq_angle_1 numeric', lambda event: event.get_pos_seq()[1]/120),\n",
      "            Field('sag_zero_seq_1 numeric', lambda event: event.get_zero_seq()[0]/mean(event.ss_mags)),\n",
      "            Field('sub_phase_21_32 numeric', lambda event: event.get_relative_phase_2_1()-event.get_relative_phase_3_2()),\n",
      "            Field('ed numeric', lambda event: event.electrical_distance),\n",
      "        ]\n",
      "\n",
      "defaultneighbourfields = [\n",
      "            Field('v_sag_n_most numeric', lambda event: min([neighbour.get_vsags()[0] for neighbour in event.neighbours])),\n",
      "            Field('v_sag_n_median numeric', lambda event: mean([neighbour.get_vsags()[0] for neighbour in event.neighbours])),\n",
      "        ]\n",
      "\n",
      "defaultlabels = [            \n",
      "            Field('nofaultvsfault {noFault, fault}', lambda event: 'noFault' if event.get_sw_annotation() == 'noFault' else 'fault'),\n",
      "            Field('groundvsnoground {SLG, noSLG}', lambda event: 'SLG' if event.get_sw_annotation() == 'SLG' else 'noSLG'),\n",
      "            Field('linevsnoline {LtL, noLtL}', lambda event: 'LtL' if event.get_sw_annotation() == 'LtL' else 'noLtL'),\n",
      "            Field('phasevsnophase {3P, no3P}', lambda event: '3P' if event.get_sw_annotation() == '3P' else 'no3P'),\n",
      "            Field('xl_annotation {SLG, LtL, 3P, noFault}', lambda event: event.get_xl_annotation()),\n",
      "            Field('sw_annotation {SLG, LtL, 3P, noFault}', lambda event: event.get_sw_annotation())\n",
      "        ]\n",
      "\n",
      "def buildArff(fn, dsname, fields, generators):\n",
      "    with open(fn, 'w') as writer:\n",
      "        writer.write('@relation '+dsname+'\\n\\n')\n",
      "        writer.write('@attribute ID numeric\\n')\n",
      "        for field in fields:\n",
      "            writer.write('@attribute ' + field.description + '\\n')\n",
      "        writer.write('\\n@data\\n')\n",
      "        index = 1\n",
      "        for _generator in generators:\n",
      "            for event in _generator:\n",
      "                arr = [str(index)]\n",
      "                for field in fields:\n",
      "                    v = field.expr(event)\n",
      "                    if isinstance(v, float): \n",
      "                        v = round(v, 4)\n",
      "                        if isnan(v):\n",
      "                            print 'Existing nan', event.eventid, event.signal, field.description\n",
      "                            break\n",
      "                        if isinf(v): \n",
      "                            print 'Existing inf', event.eventid, event.signal, field.description\n",
      "                            break\n",
      "                    arr.append(str(v))\n",
      "                if len(fields) > len(arr): continue\n",
      "                writer.write(', '.join(arr)+'\\n')\n",
      "                index += 1            \n",
      "\n",
      "class ExportingEvent(object):\n",
      "    def __init__(self, signal, phases, angles, datapoint, signaldata, distance_from_ss):\n",
      "        self.error = any([_v == 0 for s in phases for _v in datapoint[s]])\n",
      "        if self.error: pass\n",
      "\n",
      "        self.signal = signal\n",
      "        self.phases = phases\n",
      "        self.angles = angles\n",
      "        self.datapoint = datapoint\n",
      "        \n",
      "        self.init_ss_mags_(phases, signaldata)\n",
      "        self.init_ss_angle_(angles, signaldata)\n",
      "        self.distance_from_ss = distance_from_ss\n",
      "           \n",
      "    #init steady state voltage mags A B C\n",
      "    def init_ss_mags_(self, phases, signaldata):\n",
      "        self.ss_mags = array([mean(signaldata[x]) for x in phases])\n",
      "        self.std_mags = [signaldata[x][-3:]for x in phases]\n",
      "        \n",
      "    #init steady state of AOB, AOC and BOC\n",
      "    def init_ss_angle_(self, angles, signaldata):\n",
      "        angle_ab = mean([feature.relativeAngle(x, y) for x, y in zip(signaldata[angles[0]], signaldata[angles[1]])])\n",
      "        angle_ac = mean([feature.relativeAngle(x, y) for x, y in zip(signaldata[angles[0]], signaldata[angles[2]])])\n",
      "        angle_bc = mean([feature.relativeAngle(x, y) for x, y in zip(signaldata[angles[1]], signaldata[angles[2]])])\n",
      "        self.ss_angles = array([angle_ab, angle_ac, angle_bc])\n",
      "    \n",
      "    def add_neighbour(self, event):\n",
      "        if not hasattr(self, 'neighbours'): self.neighbours = []\n",
      "        self.neighbours.append(event)\n",
      "                \n",
      "    def get_vmags_(self, idx = -1):\n",
      "        return array([self.datapoint[self.phases[0]][idx], \n",
      "                          self.datapoint[self.phases[1]][idx], \n",
      "                          self.datapoint[self.phases[2]][idx]])         \n",
      "    \n",
      "    def get_angles_(self, idx = -1):\n",
      "        return array([self.datapoint[self.angles[0]][idx], \n",
      "                          self.datapoint[self.angles[1]][idx], \n",
      "                          self.datapoint[self.angles[2]][idx]]) \n",
      "    \n",
      "    def get_phase_angles_(self, idx = -1):\n",
      "        angles = self.get_angles_(idx = idx)\n",
      "        return array([feature.relativeAngle(angles[0], angles[1]),\n",
      "                       feature.relativeAngle(angles[0], angles[2]),\n",
      "                       feature.relativeAngle(angles[1], angles[2])])\n",
      "    \n",
      "    def dv(self, idx = -1):\n",
      "        vmags = self.get_vmags_(idx=idx)\n",
      "        return ((sum((1 - (vmags * 1.0 / self.ss_mags)) ** 2))**0.5)/3.0\n",
      "    \n",
      "    def get_dipped_phases_(self, idx = -1):\n",
      "        vmags = self.get_vmags_(idx = idx)\n",
      "        dipped_phases = abs(1 - vmags * 1.0 / self.ss_mags)\n",
      "        return sorted([max(x, 0.0001) for x in dipped_phases], reverse = True)\n",
      "        \n",
      "    def get_normalized_a_changes(self, idx = -1):\n",
      "        phase_angles = self.get_phase_angles_(idx = idx)\n",
      "        dipped_a_change = abs(1 - phase_angles * 1.0 / self.ss_angles)\n",
      "        return sorted([max(x, 0.0001) for x in dipped_a_change], reverse = True)\n",
      "        \n",
      "    def get_a_changes(self, idx = -1):\n",
      "        phase_angles = self.get_phase_angles_(idx = idx)\n",
      "        return sorted(phase_angles * 1.0 / self.ss_angles)\n",
      "        \n",
      "    def get_vsags(self, idx = -1):\n",
      "        vmags = self.get_vmags_(idx = idx)\n",
      "        return sorted(vmags * 1.0 / self.ss_mags)\n",
      "    \n",
      "    def get_relative_phase_2_1(self, idx = -1):\n",
      "        dipped_phase = self.get_dipped_phases_(idx = idx)\n",
      "        return dipped_phase[1]/dipped_phase[0]    \n",
      "    \n",
      "    def get_relative_phase_3_1(self, idx = -1):\n",
      "        dipped_phase = self.get_dipped_phases_(idx = idx)\n",
      "        return dipped_phase[2]/dipped_phase[0]\n",
      "    \n",
      "    def get_relative_phase_3_2(self, idx = -1):\n",
      "        dipped_phase = self.get_dipped_phases_(idx = idx)\n",
      "        return dipped_phase[2]/dipped_phase[1]\n",
      "\n",
      "    def get_neg_seq(self, idx = -1):\n",
      "        angles = self.get_angles_(idx = idx)\n",
      "        vmags = self.get_vmags_(idx = idx) \n",
      "        return feature.neg_seq(vmags[0], angles[0], vmags[1], angles[1], vmags[2], angles[2])\n",
      "    \n",
      "    def get_pos_seq(self, idx = -1):\n",
      "        angles = self.get_angles_(idx = idx)\n",
      "        vmags = self.get_vmags_(idx = idx) \n",
      "        return feature.pos_seq(vmags[0], angles[0], vmags[1], angles[1], vmags[2], angles[2])    \n",
      "    \n",
      "    def get_zero_seq(self, idx = -1):\n",
      "        angles = self.get_angles_(idx = idx)\n",
      "        vmags = self.get_vmags_(idx = idx) \n",
      "        return feature.zero_seq(vmags[0], angles[0], vmags[1], angles[1], vmags[2], angles[2])\n",
      "    \n",
      "    #avg sag last three datapoint\n",
      "    def get_avg_sags(self):\n",
      "        v_sag_a = average([i / self.ss_mags[0] for i in self.datapoint[self.phases[0]][-3:]])\n",
      "        v_sag_b = average([i / self.ss_mags[1] for i in self.datapoint[self.phases[1]][-3:]])\n",
      "        v_sag_c = average([i / self.ss_mags[2] for i in self.datapoint[self.phases[2]][-3:]])\n",
      "        return sorted([v_sag_a, v_sag_b, v_sag_c])\n",
      "    \n",
      "    def get_splot(self):\n",
      "        stats.linregress(df[phase][ss_end-4:ss_end+1],df[phase][idx-4:idx+1],)[0]\n",
      "    \n",
      "    def get_delta_sags(self):\n",
      "        vmags = self.get_vmags_(idx = -1)/self.ss_mags\n",
      "        prior_vmags = self.get_vmags_(idx = -2)/self.ss_mags\n",
      "        return sorted(vmags-prior_vmags)\n",
      "    \n",
      "  \n",
      "class MetaExportingEvent(ExportingEvent):\n",
      "    def __init__(self, eventid, signal, phases, angles, datapoint, signaldata, electrical_distance, isfaultedbus, sw_annotation, xl_annotation, distance_from_ss):\n",
      "        super(MetaExportingEvent, self).__init__(signal, phases, angles, datapoint, signaldata, distance_from_ss)\n",
      "        self.eventid = eventid\n",
      "        self.electrical_distance = electrical_distance\n",
      "        self.isfaultedbus = isfaultedbus\n",
      "        self.sw_annotation = sw_annotation\n",
      "        self.xl_annotation = xl_annotation\n",
      "        \n",
      "    def get_sw_annotation(self):\n",
      "        return common.get_generic_class(self.sw_annotation)\n",
      "    \n",
      "    def get_xl_annotation(self):\n",
      "        return common.get_generic_class(self.xl_annotation)\n",
      "    \n",
      "class Interpreter(object):\n",
      "    def __init__(self, fn, meta, data, fieldparser):\n",
      "        self.fn = fn\n",
      "        self.meta = meta\n",
      "        self.data = data\n",
      "        self.predicate = []\n",
      "        self.orderfield = ''\n",
      "        self.isdesc = False\n",
      "        self.fieldparser = fieldparser\n",
      "        self.top = -1\n",
      "\n",
      "    def __iter__(self):\n",
      "        for data in self.__select__(None):\n",
      "            yield data\n",
      "\n",
      "    def where(self, depricate):\n",
      "        self.predicate.append(self.fieldparser.lambdaparse(depricate))\n",
      "        return self\n",
      "    \n",
      "    def signal(self, signalIds):\n",
      "        exp = 'signal in [%s]' % ','.join([str(x) for x in signalIds])\n",
      "        return self.where(exp)\n",
      "    \n",
      "    def fid(self, ids):\n",
      "        exp = '( faultId , subfaultId ) in [%s]' % ','.join([str(x) for x in ids])\n",
      "        return self.where(exp)\n",
      "    \n",
      "    def notfid(self, ids):\n",
      "        exp = '( faultId , subfaultId ) not in [%s]' % ','.join([str(x) for x in ids])\n",
      "        return self.where(exp)\n",
      "    \n",
      "    def id(self, ids):\n",
      "        exp = 'ID in [%s]' % ','.join([str(x) for x in ids])\n",
      "        return self.where(exp)\n",
      "\n",
      "    def notid(self, ids):\n",
      "        exp = 'ID not in [%s]' % ','.join([str(x) for x in ids])\n",
      "        return self.where(exp)\n",
      "\n",
      "    def sort(self, fieldname, reverse = False):\n",
      "        self.orderfield = fieldname\n",
      "        self.isdesc = reverse\n",
      "        return self\n",
      "\n",
      "    def max(self, fieldname):\n",
      "        result = self.__select__(fieldname)\n",
      "        return max(self.__select__(fieldname)) if len(result) > 0 else nan\n",
      "\n",
      "    def min(self, fieldname):\n",
      "        result = self.__select__(fieldname)\n",
      "        return min(self.__select__(fieldname)) if len(result) > 0 else nan\n",
      "\n",
      "    def fieldDesc(self):\n",
      "        arr = []\n",
      "        arrfieldnames = self.__selectfieldname__(None)\n",
      "        arrfields = self.fieldparser.translatefield(arrfieldnames)\n",
      "        for attribute in self.fieldparser.getarffattributes(arrfieldnames):\n",
      "            arr.append(attribute)\n",
      "        return arr\n",
      "\n",
      "    def __len__(self):\n",
      "        result = self.get('ID')[:,0]\n",
      "        return len(result)\n",
      "\n",
      "    def __selectfieldname__(self, fieldnames):\n",
      "        if fieldnames == None: return self.fieldparser.fieldnames\n",
      "        return fieldnames.split()\n",
      "\n",
      "    def __select__(self, fieldnames):\n",
      "        X = []\n",
      "        predicate = [eval(p) for p in self.predicate]\n",
      "        for e in self.data:\n",
      "            if len(predicate) != 0 and any([p(e)==False for p in predicate]): continue\n",
      "            X.append(e)\n",
      "\n",
      "        if len(X) == 0: return []\n",
      "        if self.orderfield != '':  \n",
      "            X = sorted(X, key = itemgetter(self.fieldparser.translatefield([self.orderfield])[0]), reverse = self.isdesc)\n",
      "\n",
      "        X = array(X)\n",
      "\n",
      "        if fieldnames == None: return X\n",
      "\n",
      "        arrfieldnames = fieldnames.split()\n",
      "        arrfields = self.fieldparser.translatefield(arrfieldnames)\n",
      "        return X[:, arrfields]\n",
      "\n",
      "    def kfold(self, numFold, shuffle = False, random = 0):\n",
      "        return KFold(self, numFold, shuffle, random)\n",
      "\n",
      "    def split(self, testsize=0.2, numFold=10, shuffle = False):\n",
      "        return Splitter(self, testsize, numFold, shuffle)\n",
      "    \n",
      "    def build(self):\n",
      "        return DatasetBuilder(self)\n",
      "\n",
      "    def select(self, fieldnames=None):\n",
      "        result = self.__select__(fieldnames)\n",
      "        return result[:,:-1], result[:,-1] if len(result) > 0 else []\n",
      "    \n",
      "    def todict(self, fieldnames=None):\n",
      "        result = self.__select__(fieldnames)\n",
      "        if len(result) == 0: return []\n",
      "        fieldnames = fieldnames.split() if fieldnames != None else self.fieldparser.fieldnames\n",
      "        fieldnames = fieldnames[:-1]\n",
      "        arr = []\n",
      "        for sample in result:\n",
      "            arr.append({field:sample[idx] for idx, field in enumerate(fieldnames)})\n",
      "        return arr, result[:,-1]\n",
      "        \n",
      "    def get(self, fieldnames=None):\n",
      "        return self.__select__(fieldnames)\n",
      "\n",
      "    def save(self, fn, fieldnames=None):\n",
      "        arrfieldnames = self.__selectfieldname__(fieldnames)\n",
      "        arrfields = self.fieldparser.translatefield(arrfieldnames)\n",
      "        result = self.__select__(fieldnames)\n",
      "        with open(fn, 'w') as writer:\n",
      "            writer.write('@relation ' + self.meta + '\\n\\n')\n",
      "            for attribute in self.fieldparser.getarffattributes(arrfieldnames):\n",
      "                writer.write('@attribute %s\\n' % attribute)\n",
      "            writer.write('@data\\n\\n');\n",
      "            for e in result:\n",
      "                writer.write(', '.join([self.fieldparser.getarffvalue(e[i], index) for i, index in enumerate(arrfields)]) + '\\n')\n",
      "\n",
      "    def clone(self):\n",
      "        interpreter = Interpreter(self.fn, self.meta, self.data, self.fieldparser)\n",
      "        interpreter.predicate = list(self.predicate)\n",
      "        interpreter.orderfield = self.orderfield\n",
      "        interpreter.isdesc = self.isdesc\n",
      "        return interpreter            \n",
      "\n",
      "class FieldParser(object):\n",
      "    def __init__(self, fieldnames, datatypes):\n",
      "        self.datatypes = datatypes\n",
      "        self.fieldnames = fieldnames\n",
      "\n",
      "    def lambdaparse(self, expstr):\n",
      "        arr = expstr.split()\n",
      "        for index in range(len(arr)):\n",
      "            if arr[index] in self.fieldnames:\n",
      "                arr[index] = 'e[' + str(self.fieldnames.index(arr[index])) + ']'\n",
      "        return 'lambda e: ' + ' '.join(arr)\n",
      "\n",
      "    def translatefield(self, fieldnames):\n",
      "        return [self.fieldnames.index(fieldname) for fieldname in fieldnames]\n",
      "\n",
      "    def translatevalue(self, fieldname, value):\n",
      "        index = self.fieldnames.index(fieldname)\n",
      "        return self.datatypes[index][int(value)]\n",
      "\n",
      "    def getValue(self, fieldname, rawValue):\n",
      "        index = self.fieldnames.index(fieldname)\n",
      "        return self.datatypes[index].index(rawValue)\n",
      "\n",
      "    def getarffvalue(self, value, index):\n",
      "        if self.datatypes[index] == 1:\n",
      "            return str(value)\n",
      "        else:\n",
      "            return self.datatypes[index][int(value)]\n",
      "\n",
      "    def getarffattributes(self, fieldnames):           \n",
      "        arrfields = self.translatefield(fieldnames)\n",
      "\n",
      "        for field, dtindex in zip(fieldnames, arrfields):\n",
      "            if self.datatypes[dtindex] == 1:\n",
      "                yield '%s numeric' % field\n",
      "            else:\n",
      "                yield '%s {%s}' % (field, ', '.join(self.datatypes[dtindex]))    \n",
      "\n",
      "class DatasetBuilder(object):\n",
      "    def __init__(self, interpreter):\n",
      "        self.fielnames = []\n",
      "        self.datatypes = []\n",
      "        self.expr = []\n",
      "        self.interpreter = interpreter\n",
      "\n",
      "    def addfield(self, fielname, datatype, exp):\n",
      "        self.fielnames.append(fielname)\n",
      "        self.expr.append(exp)\n",
      "        if datatype == 'numeric': self.datatypes.append(1)\n",
      "        else: self.datatypes.append(datatype.split())\n",
      "        return self\n",
      "\n",
      "    def get(self):\n",
      "        fieldparser = FieldParser(self.fielnames, self.datatypes)\n",
      "        exp = [self.interpreter.fieldparser.lambdaparse(s) for s in self.expr]\n",
      "        data = []\n",
      "        for event in self.interpreter.__select__(None):\n",
      "            data.append([round(eval(lam)(event), 4) for lam in exp])\n",
      "        return Interpreter(self.interpreter.fn, self.interpreter.meta, array(data), fieldparser)\n",
      "    \n",
      "class Splitter():\n",
      "    def __init__(self, interpretor, testsize=0.1, numFold=10, shuffle = False):\n",
      "        self.interpretor = interpretor\n",
      "        self.stat_(testsize, numFold, shuffle, random)\n",
      "    \n",
      "    def getinterpretor(self):\n",
      "        return self.interpretor.clone()\n",
      "        \n",
      "    def stat_(self, testsize, numFold, shuffle, random):\n",
      "        raw_ids = self.interpretor.get('ID sw_annotation')\n",
      "        faultids = [[_id for _id, fault in raw_ids if fault == faulttype] for faulttype in range(4)]\n",
      "        if shuffle: \n",
      "            for subfault in faultids: random.shuffle(subfault)\n",
      "        \n",
      "        #Pop testing data with size\n",
      "        print 'size', [int(len(subfaults)) for subfaults in faultids]\n",
      "        \n",
      "        if testsize > 0.0:\n",
      "            size_test = [int(len(subfaults)*testsize + 0.4) for subfaults in faultids]\n",
      "            print 'testing pop', size_test\n",
      "            self.testingids, faultids = self.pop_(faultids, size_test)\n",
      "        else:\n",
      "            self.testingids = []\n",
      "        \n",
      "        #Pop k-fold\n",
      "        size_folds = [int(len(subfaults)/numFold*1.0 + 0.4) for subfaults in faultids]\n",
      "        self.fold_data = []\n",
      "        print 'size_folds', size_folds\n",
      "        for idx in range(numFold-1):\n",
      "            sample, faultids = self.pop_(faultids, size_folds)\n",
      "            self.fold_data.append(sample)\n",
      "        \n",
      "        self.fold_data.append(reduce(lambda x, y: list(x) + list(y), faultids))\n",
      "    \n",
      "    def kfold_data(self):\n",
      "        return self.interpretor.clone().id(reduce(lambda x, y: list(x) + list(y), list(self.fold_data)))\n",
      "\n",
      "    def pop_(self, faultids, n_samples):\n",
      "        samples = reduce(lambda x, y: x + y, [random.sample(subsample, n_sample) for subsample, n_sample in zip(faultids, n_samples)])\n",
      "        faultids = [set(subsample) - set(samples) for subsample in faultids]\n",
      "        return samples, faultids\n",
      "    \n",
      "    def get(self):\n",
      "        arr = []\n",
      "        for sample in self.fold_data:\n",
      "            arr.append((self.interpretor.clone().id(sample), self.interpretor.clone().notid(sample).notid(self.testingids))) #testing , training                \n",
      "        return arr\n",
      "    \n",
      "    def get_testing(self):\n",
      "        if len(self.testingids) == 0: return None\n",
      "        return self.interpretor.clone().id(self.testingids)     \n",
      "        \n",
      "class KFold(object):\n",
      "    def __init__(self, interpretor, numFold, shuffle = False, random = 0):\n",
      "        self.interpretor = interpretor\n",
      "        self.numFold = numFold\n",
      "        self.shuffle =  shuffle\n",
      "        self.random = random\n",
      "        pass\n",
      "\n",
      "    def __fold__(self):\n",
      "        raw_ids = self.interpretor.get('ID sw_annotation')\n",
      "        id_faulttypes = {faulttype: [_id for _id, fault in raw_ids if fault == faulttype] for faulttype in range(3)}\n",
      "        \n",
      "        random.seed(self.random)\n",
      "        \n",
      "        if self.shuffle: \n",
      "            for faultype in id_faulttypes: random.shuffle(id_faulttypes[faultype])\n",
      "        \n",
      "        size_folds = {faultype: len(id_faulttypes[faultype])/self.numFold for faultype in id_faulttypes}\n",
      "        id_faulttypes = {faultype: set(id_faulttypes[faultype]) for faultype in id_faulttypes}\n",
      "        #fold\n",
      "        self.fold_data = []\n",
      "        for n in range(self.numFold - 1):\n",
      "            sample = []\n",
      "            for faultype in id_faulttypes:\n",
      "                subsample = random.sample(id_faulttypes[faultype], size_folds[faultype])\n",
      "                id_faulttypes[faultype] -= set(subsample)\n",
      "                sample += subsample\n",
      "            self.fold_data.append(sample)\n",
      "        self.fold_data.append([item for sublist in id_faulttypes.values() for item in sublist])\n",
      "\n",
      "    def getinterpretor(self):\n",
      "        return self.interpretor.clone()\n",
      "\n",
      "    def get(self):\n",
      "        self.__fold__()\n",
      "        arr = []\n",
      "        for sample in self.fold_data:\n",
      "            arr.append((self.interpretor.clone().id(sample), self.interpretor.clone().notid(sample))) #testing , training                \n",
      "        return arr\n",
      "                    \n",
      "class Dataset(object):\n",
      "    def __parsedata__(self, fn):\n",
      "        metadata, fields, datatypes, data = '', [], [], []\n",
      "        with open(fn, 'r') as fhandler:\n",
      "            while (True):\n",
      "                try:\n",
      "                    line = fhandler.next()\n",
      "                    if line.startswith('@relation'): \n",
      "                        metadata = line.replace('@relation', '').replace('\\n', '')\n",
      "                        continue\n",
      "                    if line.startswith('@attribute'): \n",
      "                        arr = line.replace(',','').replace('{','').replace('}','').replace('\\n','').split()\n",
      "                        fields.append(arr[1])\n",
      "                        if arr[2] != 'numeric': datatypes.append(arr[2:])\n",
      "                        else: datatypes.append(1)\n",
      "                        continue\n",
      "                    if line == '@data\\n':\n",
      "                        continue\n",
      "                    if line =='\\n': continue\n",
      "                    arr = line.replace(', ', ' ').replace('\\n','').split(' ')\n",
      "                    data.append([float(value) if datatypes[index]==1 else datatypes[index].index(value) for index, value in enumerate(arr)])\n",
      "                except StopIteration:\n",
      "                    break\n",
      "        return metadata, fields, datatypes, array(data)\n",
      "    \n",
      "    def __init__(self, datafiles):\n",
      "        self.datafiles = []\n",
      "        for index, datafile in enumerate(datafiles):\n",
      "            metadata, fields, datatypes, data = self.__parsedata__(datafile)\n",
      "            self.datafiles.append([datafile, metadata, fields, datatypes, data])\n",
      "    \n",
      "    def __getitem__(self, index):\n",
      "        fieldparser = FieldParser(self.datafiles[index][2], self.datafiles[index][3])\n",
      "        return Interpreter(self.datafiles[index][0], self.datafiles[index][1], self.datafiles[index][4], fieldparser)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting arff.py\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analytics library"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile analytic.py\n",
      "from multiprocessing import Pool\n",
      "from sklearn.metrics import * \n",
      "\n",
      "# Code for Comparations of Classifiers\n",
      "def estimate(args):\n",
      "    estimator, train_set, test_set = args[0], args[1], args[2]\n",
      "    estimator.fit(train_set)\n",
      "    return [[_id, pred] for _id, pred in zip(test_set.get('ID'), estimator.predict(test_set.todict()[0]))]\n",
      "\n",
      "def cv_estimate(estimator, splitter, n_jobs = 4):\n",
      "    y_all_ids = splitter.kfold_data().get('ID sw_annotation')\n",
      "    ids = (y_all_ids[:,0].reshape(-1)).tolist()\n",
      "    y_all_test = (y_all_ids[:,1].reshape(-1)).tolist()\n",
      "    y_all_pred = (y_all_ids[:,1].reshape(-1)).tolist()\n",
      "    pool = Pool(n_jobs)\n",
      "    results = pool.map(estimate, [(estimator, train_set, test_set) for test_set, train_set in splitter.get()])\n",
      "    \n",
      "    for re in results:\n",
      "        for item in re:\n",
      "            y_all_pred[ids.index(item[0])] = item[1]\n",
      "    print 'Cross validation score', \n",
      "    print 'Confusion matrix\\n', confusion_matrix(y_all_test, y_all_pred)\n",
      "    print 'Accuracy score\\n', accuracy_score(y_all_test, y_all_pred)\n",
      "    pool.close()\n",
      "    if len(splitter.testingids) == 0: return y_all_test, y_all_pred\n",
      "    \n",
      "    estimator.fit(splitter.kfold_data())\n",
      "    y_t = splitter.get_testing().get('sw_annotation')\n",
      "    y_p = estimator.predict(splitter.get_testing().todict()[0])\n",
      "    \n",
      "    print 'Testing score'\n",
      "    print 'Confusion matrix\\n', confusion_matrix(y_t, y_p)\n",
      "    print 'Accuracy score\\n', accuracy_score(y_t, y_p)\n",
      "    \n",
      "    return y_t, y_p\n",
      "\n",
      "# Code for Nofaultvsfault_threshold_study\n",
      "def estimate1(args):\n",
      "    estimator, train_set, test_set = args[0], args[1], args[2]\n",
      "    estimator.fit(train_set)\n",
      "    return [[_id, pred] for _id, pred in zip(test_set.get('ID'), estimator.predict(test_set.todict()[0]))]\n",
      "\n",
      "def cv_estimate_kfold(estimator, splitter, n_jobs = 4):\n",
      "    y_all_ids = splitter.getinterpretor().get('ID nofaultvsfault')\n",
      "    ids = (y_all_ids[:,0].reshape(-1)).tolist()\n",
      "    y_all_test = (y_all_ids[:,1].reshape(-1)).tolist()\n",
      "    y_all_pred = (y_all_ids[:,1].reshape(-1)).tolist()\n",
      "    pool = Pool(n_jobs)\n",
      "    results = pool.map(estimate1, [(estimator, train_set, test_set) for test_set, train_set in splitter.get()])\n",
      "    \n",
      "    for re in results:\n",
      "        for item in re:\n",
      "            y_all_pred[ids.index(item[0])] = item[1]\n",
      "    print 'Cross validation score', \n",
      "    print 'Confusion matrix\\n', confusion_matrix(y_all_test, y_all_pred)\n",
      "    print 'Accuracy score\\n', accuracy_score(y_all_test, y_all_pred)\n",
      "    pool.close()\n",
      "    \n",
      "    return y_all_test, y_all_pred\n",
      "\n",
      "#http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/J48.html\n",
      "# holding a setting for training a classifer\n",
      "class WekaJ48Result:\n",
      "    @staticmethod                           \n",
      "    def writeCSVHeaderTo(writer):\n",
      "        template = 'minNumObj;properties;leaves;ground_accuracy;ground_false_neg;ground_false_pos;line_accuracy;line_false_neg;line_false_pos;phase_accuracy;phase_false_neg;phase_false_pos;nofault_accuracy;nofault_false_neg;nofault_false_pos;accuracy'\n",
      "        writer.writerow(template.split(';'))\n",
      "    \n",
      "    @staticmethod    \n",
      "    def parse(iterobj):\n",
      "        a_cfm = []\n",
      "        while (True):\n",
      "            try:\n",
      "                line = iterobj.next()\n",
      "                if (line.startswith('Options:')): \n",
      "                    strips = line.split(':')\n",
      "                    j45properties = strips[1]\n",
      "                    print 'Options:', j45properties\n",
      "                    continue\n",
      "                if (line.startswith('Number of Leaves  :')): \n",
      "                    strips = line.split(':')\n",
      "                    nLeaves = int(strips[1].strip())\n",
      "                    print 'Number of leaves:', nLeaves\n",
      "                    continue\n",
      "                if (line == '=== Confusion Matrix ==='):\n",
      "                    iterobj.next()\n",
      "                    iterobj.next()\n",
      "                    \n",
      "                    arr = [[0 for x in xrange(4)] for x in xrange(4)]\n",
      "                    for i in xrange(4):\n",
      "                        line = iterobj.next()\n",
      "                        for j, v in enumerate([int(x) for x in line.split(' ') if x.isdigit()]):\n",
      "                            arr[i][j] = v\n",
      "                    a_cfm.append(arr)\n",
      "                    continue\n",
      "            except StopIteration:\n",
      "                break       \n",
      "        return WekaJ48Result(j45properties, nLeaves, a_cfm[0], a_cfm[1])\n",
      "    \n",
      "    @property\n",
      "    def cfm(self): \n",
      "        return self._cfm\n",
      "    \n",
      "    @property\n",
      "    def cfmAccuracy(self): \n",
      "        return self._cfmAccuracy\n",
      "    \n",
      "    @property\n",
      "    def leaves(self): \n",
      "        return self._leaves\n",
      "    \n",
      "    def __init__(self, properties, leaves, accuracy, crossValidateArr):\n",
      "        self._properties = properties\n",
      "        self._leaves = leaves\n",
      "        self._cfm = cfm(crossValidateArr)\n",
      "        self._cfmAccuracy = cfm(accuracy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting analytic.py\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "decisiontree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile decisiontree.py\n",
      "from subprocess import Popen, PIPE\n",
      "from collections import namedtuple\n",
      "from arff import Dataset\n",
      "\n",
      "class DecisionNode:\n",
      "    def __init__(self, test, true, false, text):\n",
      "        self.test = test\n",
      "        self.true = true\n",
      "        self.false = false\n",
      "        self.text = text\n",
      "\n",
      "def execute(cmd):\n",
      "    p = Popen(cmd, shell = True, stdout=PIPE)\n",
      "    (output, err) = p.communicate()\n",
      "    return p.wait(), output\n",
      "\n",
      "def j48fit(input_arff, settings, verbose = False):\n",
      "    command = 'java -Xmx1000M -classpath libs/weka.jar weka.classifiers.trees.J48 -t ' + input_arff + ' ' + settings\n",
      "    if verbose: print command\n",
      "    return execute(command)\n",
      "\n",
      "def buildDecisionArray(src):\n",
      "    # define data for decision nodes\n",
      "    \n",
      "    def ismatchedFieldname(text1, text2):\n",
      "        split1 = text1.split(' ')\n",
      "        split2 = text2.split(' ')\n",
      "        return split1[0] == split2[0] and split1[2] == split2[2]\n",
      "    \n",
      "    def makeTestFunc(testFuncTxt):\n",
      "        #if (not convertLambda): return testFuncTxt\n",
      "        fieldName, operator, value = (testFuncTxt.split(' '))\n",
      "        exp = 'lambda x : x[\"{0}\"] {1} {2}'.format(fieldName, operator, value)\n",
      "        return exp\n",
      "    \n",
      "    stack = []\n",
      "    firstNode = None\n",
      "    for li, line in enumerate(src):\n",
      "        nodeInfo = (x.strip() for x in line.split('|') if len(x.strip()) > 0).next()\n",
      "        if (':' not in line): # not leaf\n",
      "            # node is an array with three values: testFunc, true statement, false statement, testFuncText\n",
      "            newnode = [makeTestFunc(nodeInfo), '', '', nodeInfo]\n",
      "            if (len(stack) == 0):\n",
      "                stack.append(newnode)\n",
      "            else:  \n",
      "                node = stack[-1]\n",
      "                if (not ismatchedFieldname(node[3], nodeInfo)):\n",
      "                    if (node[1] == ''):\n",
      "                        node[1] = newnode\n",
      "                    else:\n",
      "                        node[2] = newnode\n",
      "                        stack.pop()\n",
      "                    stack.append(newnode)\n",
      "        else:\n",
      "            testFunc, label = ([x.strip() for x in nodeInfo.split(':')])\n",
      "            newnode = [makeTestFunc(testFunc), label, '', testFunc]\n",
      "            if (len(stack) == 0):\n",
      "                # node is an array with three values: testFunc, true statement, false statement, testFuncText\n",
      "                stack.append(newnode)\n",
      "            else:\n",
      "                node = stack[-1]\n",
      "                if (node[1] == ''):\n",
      "                    node[1] = newnode\n",
      "                    stack.append(newnode)\n",
      "                else:\n",
      "                    node = stack.pop()\n",
      "                    if (ismatchedFieldname(node[3], testFunc)):\n",
      "                        node[2] = label\n",
      "                    else: \n",
      "                        node[2] = newnode\n",
      "                        stack.append(newnode)\n",
      "        if (firstNode is None):\n",
      "            firstNode = stack[0]\n",
      "    return firstNode\n",
      "\n",
      "def parse(iterobj):\n",
      "    a_cfm = []\n",
      "    tree = []\n",
      "    while (True):\n",
      "        try:\n",
      "            line = iterobj.next()\n",
      "            if (line.startswith('Options:')): \n",
      "                strips = line.split(':')\n",
      "                j45properties = strips[1]\n",
      "                print 'Options:', j45properties\n",
      "                continue\n",
      "            if (line.startswith('------------------')):\n",
      "                iterobj.next()\n",
      "                line = iterobj.next()\n",
      "                while (line != ''):\n",
      "                    tree.append(line)\n",
      "                    line = iterobj.next()\n",
      "                continue\n",
      "            if (line.startswith('Number of Leaves  :')): \n",
      "                strips = line.split(':')\n",
      "                nLeaves = int(strips[1].strip())\n",
      "                continue\n",
      "            if (line == '=== Confusion Matrix ==='):\n",
      "                iterobj.next()\n",
      "                iterobj.next()\n",
      "\n",
      "                arr = [[0 for x in xrange(4)] for x in xrange(4)]\n",
      "                for i in xrange(4):\n",
      "                    line = iterobj.next()\n",
      "                    for j, v in enumerate([int(x) for x in line.split(' ') if x.isdigit()]):\n",
      "                        arr[i][j] = v\n",
      "                a_cfm.append(arr)\n",
      "                continue\n",
      "        except StopIteration:\n",
      "            break\n",
      "    return tree\n",
      "\n",
      "def convertDecisionNode(data):\n",
      "    if (type(data) is str): return data\n",
      "    test, true, false, text =  data\n",
      "    return DecisionNode(test, convertDecisionNode(true), convertDecisionNode(false), text)\n",
      "    \n",
      "def buildDecisionNode(src):\n",
      "    return convertDecisionNode(buildDecisionArray(src))\n",
      "\n",
      "def test(decisionNode, x):\n",
      "    if (type(decisionNode) is str): return decisionNode\n",
      "    result = eval(decisionNode.test)(x)\n",
      "    return result and test(decisionNode.true, x) or test(decisionNode.false, x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting decisiontree.py\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "classifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile classifiers.py\n",
      "import numpy as np\n",
      "import common\n",
      "from sklearn.svm import SVC\n",
      "from decisiontree import *\n",
      "\n",
      "class BaseClassifier(object):\n",
      "    def get_generic_class_(self, classfication):\n",
      "        if (classfication in ['AG', 'BG', 'CG']): return 'SLG'\n",
      "        if (classfication in ['AB', 'BA', 'BC', 'BC', 'AC', 'CA']): return 'LtL'\n",
      "        if (classfication in ['ABC']): return '3P'\n",
      "        return 'noFault'\n",
      "    \n",
      "    def translate_(self, classfication):\n",
      "        if (classfication in ['AG', 'BG', 'CG', 'SLG']): return 0.0\n",
      "        if (classfication in ['AB', 'BA', 'BC', 'BC', 'AC', 'CA', 'LtL']): return 1.0\n",
      "        if (classfication in ['ABC', '3P']): return 2.0\n",
      "        return 3.0\n",
      "    \n",
      "    def vote_predict_by_fault_(self, faults, X):\n",
      "        results = {}\n",
      "        classification = []\n",
      "        for x, y_p in zip(X, self.predict(X)):\n",
      "            fid = x['faultId'], x['subfaultId']\n",
      "            if fid not in results: results[fid] = []\n",
      "            results[fid].append(y_p)\n",
      "    \n",
      "        for fid in faults:\n",
      "            if fid not in results: raise Exception('No data for (%d, %d)' % (fid[0], fid[1]))\n",
      "            if 3 in results[fid] and len(results[fid]) > 1: results[fid].remove(3)\n",
      "            classification.append([_c for _c in results[fid]])\n",
      "        return classification\n",
      "    \n",
      "    def predict(self, X):\n",
      "        return [self.predict_instance(e) for e in X]\n",
      "    \n",
      "    def decision_function(self, X):\n",
      "        return [self.decision_function_instance(e) for e in X]\n",
      "    \n",
      "    def predict_instance(self, X): pass\n",
      "    \n",
      "    def decision_function_instance(self, X): pass\n",
      "    \n",
      "    def features(self): pass\n",
      "    \n",
      "    def __init__(self): \n",
      "        pass\n",
      "    \n",
      "    def __str__(self): pass\n",
      "\n",
      "class XLVoltageSagClassifier(BaseClassifier):\n",
      "    def vote_predict_by_fault(self, faults, X):\n",
      "        return BaseClassifier.vote_predict_by_fault_(self, faults, X)        \n",
      "    \n",
      "    def is_slg_(self, sags):\n",
      "        if sags[0] <= 0.95 and sags[1] >= .93 and sags[2] >= .93: return True\n",
      "        return False\n",
      "\n",
      "    def is_ll_(self, sags):\n",
      "        if sags[0] >= 0.35 and sags[1] >= 0.35:\n",
      "            if sags[0] <= 0.93 and sags[1] <= 0.93:\n",
      "                if sags[2] > 0.88: return True\n",
      "        return False\n",
      "\n",
      "    def is_tp_(self, sags):\n",
      "        if np.all([s >= 0 for s in sags]):\n",
      "            if np.all([s <= .8 for s in sags]): \n",
      "                if sags.std() < 0.05 * sags.mean():\n",
      "                    return True\n",
      "        return False\n",
      "    \n",
      "    def predict_instance(self, X):\n",
      "        sags = np.array([X[f] for f in self.features()]) # ascending order\n",
      "        if self.is_slg_(sags): return self.translate_('SLG')\n",
      "        elif self.is_ll_(sags): return self.translate_('LtL')\n",
      "        elif self.is_tp_(sags): return self.translate_('3P')\n",
      "        return self.translate_('noFault')\n",
      "    \n",
      "    def features(self):\n",
      "        return ['v_sag_1', 'v_sag_2', 'v_sag_3']\n",
      "    \n",
      "    def __init__(self):\n",
      "        super(XLVoltageSagClassifier, self).__init__()\n",
      "        \n",
      "class XLRevisedVoltageSagClassifier(XLVoltageSagClassifier):\n",
      "    def is_slg_(self, sags):\n",
      "        if sags[0] <= 0.95 and sags[1] >= .95 and sags[2] >= .95: return True\n",
      "        return False\n",
      "\n",
      "    def is_ll_(self, sags):\n",
      "        if sags[0] >= 0.35 and sags[1] >= 0.35:\n",
      "            if sags[0] <= 0.95 and sags[1] <= 0.95:\n",
      "                if sags[2] > 0.95: return True\n",
      "        return False\n",
      "\n",
      "    def is_tp_(self, sags):\n",
      "        if np.all([s >= 0 for s in sags]):\n",
      "            if np.all([s <= .8 for s in sags]):\n",
      "                sag_mean = sags.mean() \n",
      "                dsag = [(s - sag_mean) for s in sags]\n",
      "                dsag_abs = [abs(d) for d in dsag]\n",
      "                dsag_absmax = max(dsag_abs)\n",
      "                if (dsag_absmax/sag_mean) <= .1:\n",
      "                    return True\n",
      "        return False\n",
      "    \n",
      "    def __init__(self):\n",
      "        super(XLRevisedVoltageSagClassifier, self).__init__()\n",
      "\n",
      "class XLRuleBasedClassifier(BaseClassifier):\n",
      "    def vote_predict_by_fault(self, faults, X):\n",
      "        return BaseClassifier.vote_predict_by_fault_(self, faults, X)    \n",
      "               \n",
      "    def predict_instance(self, x):\n",
      "        _F = ['F']\n",
      "        #3P\n",
      "        if x['r1'] > 50:\n",
      "            if x['change_neg_seq_neg_peak_a_change'] > 100 or x['change_zero_seq_neg_peak_a_change'] > 100:\n",
      "                _F.append('ABC')\n",
      "\n",
      "        #LtL\n",
      "        LLVotes = []\n",
      "        per_phase_sags = [np.abs(x['adiff']), np.abs(x['bdiff']), np.abs(x['cdiff'])]\n",
      "        sper_phase_sags = sorted(per_phase_sags[:])\n",
      "        if x['zdiff'] < 0.004: LLVotes.append('R1.1')\n",
      "        else:\n",
      "            if sper_phase_sags[1] > .1:\n",
      "                if np.abs(x['zdiff']) < np.abs(x['ndiff']):\n",
      "                    LLVotes.append('R1.2')\n",
      "        if x['change_neg_seq_neg_peak_a_change'] > 100 or x['change_zero_seq_neg_peak_a_change'] > 100:\n",
      "            LLVotes.append('R2')\n",
      "        if x['r1'] < 50:\n",
      "            LLVotes.append('R3')\n",
      "        if np.abs(x['pdiff']/x['ndiff']) < 3.0:\n",
      "            LLVotes.append('R4')\n",
      "        if len(LLVotes) == 4:\n",
      "            if sper_phase_sags[1] > .1:\n",
      "                _fault = \"\"\n",
      "                if sper_phase_sags[2] == per_phase_sags[0]:\n",
      "                    _fault = \"A\"\n",
      "                elif sper_phase_sags[2] == per_phase_sags[1]:\n",
      "                    _fault = \"B\"\n",
      "                elif sper_phase_sags[2] == per_phase_sags[2]:\n",
      "                    _fault = \"C\"\n",
      "                if sper_phase_sags[1] == per_phase_sags[0]:\n",
      "                    _fault = _fault + \"A\"\n",
      "                elif sper_phase_sags[1] == per_phase_sags[1]:\n",
      "                    _fault = _fault + \"B\"\n",
      "                elif sper_phase_sags[1] == per_phase_sags[2]:\n",
      "                    _fault = _fault + \"C\"\n",
      "                _F.append(_fault)\n",
      "\n",
      "        # XL SLG Fault:\n",
      "        SLGVotes = []\n",
      "        if np.abs(x['zdiff']) > np.abs(x['ndiff']): \n",
      "            SLGVotes.append('R1.1')\n",
      "        else:\n",
      "            if sper_phase_sags[2] > .1 and sper_phase_sags[1] < .1:\n",
      "                if x['nm_nmdiff']/x['zm_nmdiff'] < 4:\n",
      "                    SLGVotes.append('R1.2')\n",
      "        if x['change_neg_seq_neg_peak_a_change'] > 100 or x['change_zero_seq_neg_peak_a_change'] > 100:\n",
      "            SLGVotes.append('R2')\n",
      "        if x['r1'] < 50:\n",
      "            SLGVotes.append('R3')\n",
      "        if np.abs(x['pdiff']/x['ndiff']) < 3.0:\n",
      "            SLGVotes.append('R4')\n",
      "        if len(SLGVotes) == 4:\n",
      "            if sper_phase_sags[2] > .1 and sper_phase_sags[1] < .1:\n",
      "                if sper_phase_sags[2] == per_phase_sags[0]:\n",
      "                    _F.append('AG')\n",
      "                elif sper_phase_sags[2] == per_phase_sags[1]:\n",
      "                    _F.append('BG')\n",
      "                elif sper_phase_sags[2] == per_phase_sags[2]:\n",
      "                    _F.append('CG')\n",
      "\n",
      "        # Look at the classification results -- if we've added a classification \n",
      "        # to the default value 'F', then we can remove the 'F'... Since the rules\n",
      "        # are not mutually exclusive, we may have more than one classification\n",
      "        if len(_F) > 1 and \"F\" in _F: _F.remove('F')\n",
      "        return self.translate_(_F[0])\n",
      "\n",
      "class XLRevisedRuleBasedClassifier(XLRuleBasedClassifier):\n",
      "    \n",
      "    def predict_instance(self, x):\n",
      "        _F = ['F']\n",
      "        #3P\n",
      "        if x['r1'] > 50:\n",
      "            if x['change_neg_seq_max_a_change'] > 100 or x['change_zero_seq_max_a_change'] > 100:\n",
      "                _F.append('ABC')\n",
      "\n",
      "        #LtL\n",
      "        LLVotes = []\n",
      "        per_phase_sags = [np.abs(x['adiff']), np.abs(x['bdiff']), np.abs(x['cdiff'])]\n",
      "        sper_phase_sags = sorted(per_phase_sags[:])\n",
      "        if x['zdiff'] < 0.004: LLVotes.append('R1.1')\n",
      "        else:\n",
      "            if sper_phase_sags[1] > .1:\n",
      "                if np.abs(x['zdiff']) < np.abs(x['ndiff']):\n",
      "                    LLVotes.append('R1.2')\n",
      "        if x['change_neg_seq_max_a_change'] > 100 or x['change_zero_seq_max_a_change'] > 100:\n",
      "            LLVotes.append('R2')\n",
      "        if x['r1'] < 50:\n",
      "            LLVotes.append('R3')\n",
      "        if np.abs(x['pdiff']/x['ndiff']) < 3.0:\n",
      "            LLVotes.append('R4')\n",
      "        if (len(LLVotes) == 4) or ('R1.1' in LLVotes):\n",
      "            if sper_phase_sags[1] > .1:\n",
      "                _fault = \"\"\n",
      "                if sper_phase_sags[2] == per_phase_sags[0]:\n",
      "                    _fault = \"A\"\n",
      "                elif sper_phase_sags[2] == per_phase_sags[1]:\n",
      "                    _fault = \"B\"\n",
      "                elif sper_phase_sags[2] == per_phase_sags[2]:\n",
      "                    _fault = \"C\"\n",
      "                if sper_phase_sags[1] == per_phase_sags[0]:\n",
      "                    _fault = _fault + \"A\"\n",
      "                elif sper_phase_sags[1] == per_phase_sags[1]:\n",
      "                    _fault = _fault + \"B\"\n",
      "                elif sper_phase_sags[1] == per_phase_sags[2]:\n",
      "                    _fault = _fault + \"C\"\n",
      "                _F.append(_fault)\n",
      "\n",
      "        # XL SLG Fault:\n",
      "        SLGVotes = []\n",
      "        if np.abs(x['zdiff']) > np.abs(x['ndiff']): \n",
      "            SLGVotes.append('R1.1')\n",
      "        else:\n",
      "            if sper_phase_sags[2] > .1 and sper_phase_sags[1] < .1:\n",
      "                if x['nm_nmdiff']/x['zm_nmdiff'] < 4:\n",
      "                    SLGVotes.append('R1.2')\n",
      "        if x['change_neg_seq_max_a_change'] > 100 or x['change_zero_seq_max_a_change'] > 100:\n",
      "            SLGVotes.append('R2')\n",
      "        if x['r1'] < 50:\n",
      "            SLGVotes.append('R3')\n",
      "        if np.abs(x['pdiff']/x['ndiff']) < 3.0:\n",
      "            SLGVotes.append('R4')\n",
      "        if (len(SLGVotes) == 4) or ('R1.1' in SLGVotes):\n",
      "            if sper_phase_sags[2] > .1 and sper_phase_sags[1] < .1:\n",
      "                if sper_phase_sags[2] == per_phase_sags[0]:\n",
      "                    _F.append('AG')\n",
      "                elif sper_phase_sags[2] == per_phase_sags[1]:\n",
      "                    _F.append('BG')\n",
      "                elif sper_phase_sags[2] == per_phase_sags[2]:\n",
      "                    _F.append('CG')\n",
      "\n",
      "        # Look at the classification results -- if we've added a classification \n",
      "        # to the default value 'F', then we can remove the 'F'... Since the rules\n",
      "        # are not mutually exclusive, we may have more than one classification\n",
      "        if len(_F) > 1 and \"F\" in _F: _F.remove('F')\n",
      "        return self.translate_(_F[0])\n",
      "\n",
      "class J48Classifier(BaseClassifier):\n",
      "    def __init__(self, M):\n",
      "        super(J48Classifier, self).__init__()\n",
      "        self.j48properties = '-R -N 10 -Q 5 -M %d' % M\n",
      "        self.feature = 'v_sag_1 rel_phase_2_1 rel_phase_3_1 sw_annotation'\n",
      "        self.temppath = 'temp/dataset_run.arff'\n",
      "        self.decisionNode = None\n",
      "    \n",
      "    def features(self):\n",
      "        return self.feature.split()[:-1]\n",
      "    \n",
      "    def fit(self, interpretor):\n",
      "        interpretor.save(self.temppath, self.feature)\n",
      "        exit_code, output = j48fit(self.temppath, self.j48properties)\n",
      "        self.decisionNode = buildDecisionNode(parse(iter(output.split('\\n'))))\n",
      "        print output\n",
      "    \n",
      "    def predict_instance(self, X):\n",
      "        pred = test(self.decisionNode, X).split()[0]\n",
      "        return self.translate_(pred)\n",
      "    \n",
      "    def update(self, decisionNode):\n",
      "        self.decisionNode = decisionNode\n",
      "    \n",
      "    def __str__(self):\n",
      "        return 'J48Classifier from IGSC15'\n",
      "    \n",
      "    \n",
      "# please do not change since it for stage gate\n",
      "class EventClassifier(BaseClassifier):\n",
      "    @classmethod\n",
      "    def LoadPickle(cls, pickle_fn):\n",
      "        return common.loadpickle(pickle_fn)\n",
      "    \n",
      "    def __init__(self):\n",
      "        super(EventClassifier, self).__init__()\n",
      "        self.nofaultclassifier = SVC(kernel='poly', degree=8, C = 1, coef0=1.2) # can be set to 8-10 with ss check  \n",
      "        self.phaseclassifier = SVC(kernel='linear', C=0.1, coef0=1)    \n",
      "        self.slgclassifier = SVC(kernel='poly', degree=3, C=0.1, coef0=1)\n",
      "        self.faultfeatures = 'ID v_sag_1 a_change_1 nofaultvsfault'\n",
      "        self.phasefeatures = 'ID rel_phase_3_1 rel_phase_3_2 phasevsnophase'\n",
      "        self.slgfeatures = 'ID rel_phase_2_1 sub_phase_2_1 v_sag_2 groundvsnoground'\n",
      "        pass\n",
      "    \n",
      "    def features(self):\n",
      "        return self.faultfeatures.split()[1:-1] + self.phasefeatures.split()[1:-1] + self.slgfeatures.split()[1:-1]\n",
      "    \n",
      "    def fit(self, interpretor):\n",
      "        X, Y = interpretor.select(self.faultfeatures)\n",
      "        self.nofaultclassifier = self.nofaultclassifier.fit(X[:,1:], Y)\n",
      "        ids = [x[0]for x, y, y_p in zip(X, Y, self.nofaultclassifier.predict(X[:,1:])) if y == y_p == 1.0]\n",
      "\n",
      "        X, Y = interpretor.clone().id(ids).select(self.phasefeatures)\n",
      "        self.phaseclassifier = self.phaseclassifier.fit(X[:,1:], Y)\n",
      "        ids = [x[0]for x, y, y_p in zip(X, Y, self.phaseclassifier.predict(X[:,1:])) if y == y_p == 1.0]\n",
      "        \n",
      "        X, Y = interpretor.clone().id(ids).select(self.slgfeatures)\n",
      "        self.slgclassifier = self.slgclassifier.fit(X[:,1:], Y)\n",
      "        \n",
      "    def get_decision_(self, y_fault, y_phase, y_slg):\n",
      "        if y_fault[0] == 0: return 3.0\n",
      "        if y_phase[0] == 0: return 2.0\n",
      "        return y_slg[0] \n",
      "       \n",
      "    def predict_instance(self, X):\n",
      "        y_f_pred = self.nofaultclassifier.predict([X[key] for key in self.faultfeatures.split()[1:-1]])\n",
      "        y_p_pred = self.phaseclassifier.predict([X[key] for key in self.phasefeatures.split()[1:-1]])\n",
      "        y_s_pred = self.slgclassifier.predict([X[key] for key in self.slgfeatures.split()[1:-1]])\n",
      "        return self.get_decision_(y_f_pred, y_p_pred, y_s_pred)\n",
      "    \n",
      "    def __str__(self):\n",
      "        s = []\n",
      "        \n",
      "        s.append('EventClassifier')\n",
      "        \n",
      "        s.append('classifier.nofaultclassifier: %s' % self.nofaultclassifier)\n",
      "        s.append('\\t%s' % self.faultfeatures)\n",
      "        \n",
      "        s.append('classifier.phaseclassifier: %s' % self.phaseclassifier)\n",
      "        s.append('\\t%s' % self.phasefeatures)\n",
      "        \n",
      "        s.append('classifier.slgclassifier: %s' % self.slgclassifier)\n",
      "        s.append('\\t%s' % self.slgfeatures)        \n",
      "    \n",
      "        return '\\n'.join(s)\n",
      "\n",
      "class ProxyEventClassifier(BaseClassifier):\n",
      "    def __init__(self, cls, features):\n",
      "        super(ProxyEventClassifier, self).__init__()\n",
      "        self.cls = cls\n",
      "        self._f = features\n",
      "    \n",
      "    def features(self): return self._f\n",
      "    \n",
      "    def fit(self, interpretor): \n",
      "        X, y = interpretor.select(self._f.replace('ID ',''))\n",
      "        self.cls = self.cls.fit(X, y)\n",
      "    \n",
      "    def predict_instance(self, X):\n",
      "        return self.cls.predict([X[key] for key in self._f.split()[1:-1]])[0]\n",
      "    \n",
      "    def decision_function_instance(self, X):\n",
      "        return self.cls.decision_function([X[key] for key in self._f.split()[1:-1]])[0]    \n",
      "    \n",
      "    def __str__(self):\n",
      "        s = []\n",
      "        \n",
      "        s.append('ProxyEventClassifier')\n",
      "        s.append('classifier: %s' % self.cls)\n",
      "        s.append('\\t%s' % self._f)\n",
      "\n",
      "        return '\\n'.join(s)        \n",
      "    \n",
      "class FilteredEventClassifier(BaseClassifier):  \n",
      "    def __init__(self, cls, signal_ids):\n",
      "        super(FilteredEventClassifier, self).__init__()\n",
      "        self.signal_ids = signal_ids\n",
      "        self.cls = cls\n",
      "        pass\n",
      "    \n",
      "    def features(self): return self.cls.features\n",
      "\n",
      "    def fit(self, interpretor):\n",
      "        self.cls.fit(interpretor.signal(self.signal_ids))\n",
      "        \n",
      "    def predict_instance(self, X):\n",
      "        return self.cls.predict_instance(X)\n",
      "    \n",
      "    def __str__(self):\n",
      "        s = []\n",
      "        s.append('FilteredEventClassifier')\n",
      "        s.append(self.cls)\n",
      "        return '\\n'.join(s)\n",
      "    \n",
      "class EventClassifier_20150827(EventClassifier):\n",
      "    \"\"\"\n",
      "    Classifier is using 'arff/sag_mode_0_98 datasets'\n",
      "    \"\"\"\n",
      "    def __init__(self, vsag1_threshold = None):\n",
      "        super(EventClassifier_20150827, self).__init__()\n",
      "        self.nofaultclassifier = SVC(kernel='poly', degree=7.5, C = 1., coef0=1.2) # can be set to 8-10 with ss check  \n",
      "        self.phaseclassifier = SVC(kernel='poly', degree=2, C = 1., coef0=1.2)    \n",
      "        self.slgclassifier = SVC(kernel='poly', degree=2, C = 1., coef0=1.2)\n",
      "        self.faultfeatures = 'ID v_sag_1 a_change_1 nofaultvsfault'\n",
      "        self.phasefeatures = 'ID rel_phase_3_1 rel_phase_3_2 phasevsnophase'\n",
      "        self.slgfeatures = 'ID a_change_2 rel_phase_2_1 groundvsnoground'\n",
      "        self.vsag1_threshold = vsag1_threshold\n",
      "        \n",
      "    def fit (self, interpretor):\n",
      "        super(EventClassifier_20150827, self).fit(interpretor)\n",
      "        if self.vsag1_threshold is None: return\n",
      "        X, Y = interpretor.clone().where('sw_annotation == 3.0 or v_sag_1 <= %f ' % self.vsag1_threshold).select(self.faultfeatures)\n",
      "        self.nofaultclassifier = self.nofaultclassifier.fit(X[:,1:], Y)\n",
      "        \n",
      "class EventClassifier_20160101(EventClassifier_20150827):\n",
      "    \"\"\"\n",
      "    Classifier is using 'arff/sag_mode_0_98 datasets'\n",
      "    \"\"\"\n",
      "    def __init__(self, vsag1_threshold = None):\n",
      "        super(EventClassifier_20160101, self).__init__(vsag1_threshold)\n",
      "        #can combine 3p and ltl with poly degree 3.5\n",
      "        self.nofault_vs_ltl_classifier = SVC(kernel='poly', degree=7.5, C = 1., coef0=1.2) # can be set to 8-10 with ss check\n",
      "        self.nofault_vs_3p_classifier = SVC(kernel='poly', degree=7.5, C = 1., coef0=1.2) # can be set to 8-10 with ss check\n",
      "        self.nofault_vs_slg_classifier = SVC(kernel='poly', degree=7.5, C = 1., coef0=1.2) # can be set to 8-10 with ss check  \n",
      "        \"\"\"\n",
      "        self.phaseclassifier = SVC(kernel='poly', degree=2, C = 1., coef0=1.)    \n",
      "        self.slgclassifier = SVC(kernel='poly', degree=2, C = 1., coef0=1.)\n",
      "        self.faultfeatures = 'ID avg_sag_1 delta_sag_1  a_change_1 nofaultvsfault'\n",
      "        self.phasefeatures = 'ID rel_phase_3_1 rel_phase_3_2 phasevsnophase'\n",
      "        self.slgfeatures = 'ID a_change_2 rel_phase_2_1 groundvsnoground'\n",
      "        \"\"\"\n",
      "        \n",
      "    def predict_(self, clf, X, feature):\n",
      "        return clf.predict([X[key] for key in feature.split()[1:-1]])\n",
      "        \n",
      "    def fit(self, interpretor):\n",
      "        #training nofault_vs_ltl_classifier\n",
      "        interpretor_t = interpretor.clone()\n",
      "        if self.vsag1_threshold is not None:\n",
      "            interpretor_t = interpretor_t.where('sw_annotation == 3.0 or v_sag_1 <= %f'%self.vsag1_threshold)\n",
      "\n",
      "        X, Y = interpretor_t.clone().where('sw_annotation in {1, 3}').select(self.faultfeatures)\n",
      "        self.nofault_vs_ltl_classifier.fit(X[:,1:], Y)\n",
      "        \n",
      "        #training nofault_vs_ltl_classifier\n",
      "        X, Y = interpretor_t.clone().where('sw_annotation in {2, 3}').select(self.faultfeatures)\n",
      "        self.nofault_vs_3p_classifier.fit(X[:,1:], Y)        \n",
      "        \n",
      "        #training nofault_vs_slg_classifier\n",
      "        X, Y = interpretor_t.clone().where('sw_annotation in {0, 3}').select(self.faultfeatures)\n",
      "        self.nofault_vs_slg_classifier.fit(X[:,1:], Y)\n",
      "        \n",
      "        # training phaseclassifier\n",
      "        X, Y = interpretor.clone().where('sw_annotation in {0, 1, 2}').select(self.phasefeatures)\n",
      "        self.phaseclassifier = self.phaseclassifier.fit(X[:,1:], Y)\n",
      "        \n",
      "        # training slgclassifier\n",
      "        ids = [x[0] for x, y, y_p in zip(X, Y, self.phaseclassifier.predict(X[:,1:])) if y == y_p == 1.0]\n",
      "        X, Y = interpretor.clone().id(ids).select(self.slgfeatures)\n",
      "        self.slgclassifier = self.slgclassifier.fit(X[:,1:], Y)\n",
      "    \n",
      "    def predict_instance(self, X):       \n",
      "        if self.predict_(self.phaseclassifier, X, self.phasefeatures) == 0.0:\n",
      "            if self.predict_(self.nofault_vs_3p_classifier, X, self.faultfeatures) == 0.0: return 3.0\n",
      "            else: return 2.0\n",
      "        elif self.predict_(self.slgclassifier, X, self.slgfeatures) == 0.0:\n",
      "            if self.predict_(self.nofault_vs_slg_classifier, X, self.faultfeatures) == 0.0: return 3.0\n",
      "            else: return 0.0\n",
      "        else:\n",
      "            if self.predict_(self.nofault_vs_ltl_classifier, X, self.faultfeatures) == 0.0: return 3.0\n",
      "            else: return 1.0\n",
      "            \n",
      "    def __str__(self):\n",
      "        s = []\n",
      "        \n",
      "        s.append('EventClassifier_20160101')\n",
      "        \n",
      "        s.append('classifier.nofaultclassifier: %s' % self.nofault_vs_ltl_classifier)\n",
      "        s.append('\\t%s' % self.faultfeatures)\n",
      "        \n",
      "        s.append('classifier.phaseclassifier: %s' % self.phaseclassifier)\n",
      "        s.append('\\t%s' % self.phasefeatures)\n",
      "        \n",
      "        s.append('classifier.slgclassifier: %s' % self.slgclassifier)\n",
      "        s.append('\\t%s' % self.slgfeatures)        \n",
      "    \n",
      "        return '\\n'.join(s)            \n",
      "    \n",
      "# please do not change since it for stage gate\n",
      "class SeriesEventClassifier(EventClassifier):\n",
      "    \"\"\"\n",
      "    Classifier is using 'arff/sag_mode_0_98_series datasets'\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        super(SeriesEventClassifier, self).__init__()\n",
      "        self.nofaultclassifier = SVC(kernel='poly', degree=14, C=1, coef0=1.5)\n",
      "        self.faultfeatures = 'ID v_sag_1 avg_sag_1 nofaultvsfault' \n",
      "        \n",
      "        self.slgclassifier = SVC(kernel='poly', degree=19, C=1, coef0=1.2)\n",
      "        self.slgfeatures = 'ID rel_phase_2_1 avg_sag_2 groundvsnoground'\n",
      "        pass\n",
      "    \n",
      "class SeriesEventClassifier1(EventClassifier):\n",
      "    \"\"\"\n",
      "    Classifier is using 'arff/sag_mode_0_98_series datasets'\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        super(SeriesEventClassifier1, self).__init__()\n",
      "        self.nofaultclassifier = SVC(kernel='poly', degree=16, C=1, coef0=1.5)\n",
      "        self.faultfeatures = 'ID avg_sag_1 delta_sag_1 nofaultvsfault' \n",
      "        \n",
      "        self.slgclassifier = SVC(kernel='poly', degree=19, C=1, coef0=1.2)\n",
      "        self.slgfeatures = 'ID rel_phase_2_1 avg_sag_2 groundvsnoground'\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting classifiers.py\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "datafiles"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile datafiles.py\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import classifyfault_v1\n",
      "import csv, re\n",
      "\n",
      "from libs import common\n",
      "from bpati import pmu\n",
      "from bpati import signals\n",
      "\n",
      "from os import listdir\n",
      "from os.path import join, isfile\n",
      "\n",
      "class DataFiles:\n",
      "    # option indicate how pmu object is initiated\n",
      "    # 0: local using minute20.h5\n",
      "    # 1: 2013_zeta using minutes.h5\n",
      "    # 2: 2014_qnap using 2014 access method\n",
      "    def __init__(self, option):\n",
      "        self.valid_faults = set(['AG', 'BG', 'CG', 'AB', 'BC', 'AC', 'ABC'])\n",
      "        if option == 0:\n",
      "            BPA_DATA = \"pmu-data/\"\n",
      "            self.pmudata = pmu.PMUData(pmusearchpath=BPA_DATA,\n",
      "                               pmudata=[\"minutes20.h5\"],\n",
      "                               eventdata=BPA_DATA+\"precise-event-signatures.csv\")\n",
      "            self.hdf5 = BPA_DATA+\"minutes20.h5\"\n",
      "        elif option == 1:\n",
      "            BPA_DATA = \"../../../../BPA-PMU/data/\"\n",
      "            self.pmudata = pmu.PMUData(pmusearchpath=BPA_DATA+\"hdf5/\",\n",
      "                               pmudata=[\"minutes.h5\"],\n",
      "                               eventdata=BPA_DATA+\"events/precise-event-signatures.csv\")\n",
      "            self.hdf5 = BPA_DATA+\"hdf5/minutes.h5\"\n",
      "        else:\n",
      "            BPA_DATA14 = \"/mnt/qnap/BPATI_2014\"\n",
      "            self.pmudata = pmu.PMUData(pmusearchpath=BPA_DATA14,\n",
      "                               pmudata=['2014.cmap'],\n",
      "                               configcachepath='new-configs',\n",
      "                               configcachesuffix='-.ncfg')\n",
      "        \n",
      "        if option != 2:\n",
      "            self.electrical_sites, self.electrical_distances = self.init_distance_matrix_('txt/electrical_dist_500kv_sites.csv')\n",
      "            self.hop_sites, self.hop_distances = self.init_distance_matrix_('txt/site_matrix_distance.csv')\n",
      "            self.filterhandler = lambda x: any([x.startswith(s) for s in signals.SITES_2013])\n",
      "            self.sitetranslator = lambda x: [site for site in signals.SITES_2013 if x.startswith(site)][0]            \n",
      "            self.get_distance = self.get_distance_\n",
      "            self.get_electrical_distance = self.get_electrical_distance_\n",
      "            self.get_hop_distance = self.get_hop_distance_\n",
      "            self.get_faults_signature = self.get_faults_signature_\n",
      "            self.get_sw_faults = self.get_sw_faults_\n",
      "            self.get_xl_faults = self.get_xl_faults_\n",
      "            self.get_normal_minutes = self.get_normal_minutes_\n",
      "            self.get_smag = self.get_smag_\n",
      "            self.get_signal_id_by = self.get_signal_id_by_\n",
      "            self.get_neigbour_signals = self.get_neigbour_signals_\n",
      "        else:\n",
      "            self.get_faults_signature = self.get_faults_signature_2014_\n",
      "            self.get_smag = self.get_smag_2014_\n",
      "            \n",
      "        pass\n",
      "    \n",
      "    def init_distance_matrix_(self, fn):\n",
      "        distances = {}\n",
      "        sites = []\n",
      "        with open(fn, 'r') as csvfile:\n",
      "            csvreader = csv.reader(csvfile, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
      "            for rowIndex, row in enumerate(csvreader):\n",
      "                if rowIndex == 0:\n",
      "                    sites = row[1:]\n",
      "                    continue\n",
      "                for colIndex in range(1,len(row)):\n",
      "                    if sites[rowIndex-1] == sites[colIndex-1]: continue\n",
      "                    distances[(sites[rowIndex-1], sites[colIndex-1])] = round(float(row[colIndex]), 8)\n",
      "        return sites, distances\n",
      "    \n",
      "    #option0: electrical distance\n",
      "    #option1: hop distance\n",
      "    def get_distance_(self, signal1, signal2, option=0):\n",
      "        if signal1 == signal2: return 0\n",
      "        if self.sitetranslator(signal1) == self.sitetranslator(signal2): return 0\n",
      "        \n",
      "        if option == 0 and (self.sitetranslator(signal1) not in self.electrical_sites or \\\n",
      "                                self.sitetranslator(signal2) not in self.electrical_sites): return -1\n",
      "        if option == 1 and (self.sitetranslator(signal1) not in self.hop_sites or \\\n",
      "                                self.sitetranslator(signal2) not in self.hop_sites): return -1\n",
      "        \n",
      "        if option == 0: return self.electrical_distances[self.sitetranslator(signal1), self.sitetranslator(signal2)]\n",
      "        return self.hop_distances[self.sitetranslator(signal1), self.sitetranslator(signal2)]\n",
      "\n",
      "    def get_neigbour_signals_(self, signal, n_neigbour=2, predicator=None):\n",
      "        neigbour_signals = []\n",
      "        signals, _, _ = self.get_smag()\n",
      "        neigbour_bus = []\n",
      "        neigbour_sites = []\n",
      "        \n",
      "        for x in signals:\n",
      "            if x == signal or self.sitetranslator(x) == self.sitetranslator(signal): continue\n",
      "            if self.sitetranslator(x) in neigbour_sites: continue\n",
      "            ed, hop = self.get_distance_(x, signal, 0), self.get_distance_(x, signal, 1)       \n",
      "            if (not predicator(x, ed, hop)): continue\n",
      "            neigbour_bus.append((hop, x))\n",
      "            neigbour_sites.append(self.sitetranslator(x))\n",
      "            if len(neigbour_bus) == n_neigbour: break\n",
      "        \n",
      "        return sorted(neigbour_bus)    \n",
      "    \n",
      "    def get_electrical_distance_(self, signal1, signal2):\n",
      "        return self.get_distance_(signal1, signal2, option=0)\n",
      "\n",
      "    def get_hop_distance_(self, signal1, signal2):\n",
      "        return self.get_distance_(signal1, signal2, option=1)\n",
      "\n",
      "    def get_faults_signature_(self):\n",
      "        faults = {}\n",
      "        for faultdata in self.pmudata.events():\n",
      "            faults[faultdata['id']] = [faultdata['xl_annotation'], faultdata['sw_annotation']]\n",
      "        return faults\n",
      "    \n",
      "    def get_faults_signature_2014_(self, month, year):\n",
      "        return pd.read_csv('/mnt/qnap/events%d/events%d%d.csv' % (year,year,month), \n",
      "                           converters = {'precise start time': common.date_time_converter,\n",
      "                                         'precise end time': common.date_time_converter,\n",
      "                                         'OutDatetime': common.date_time_utc_converter,\n",
      "                                         'InDatetime': common.date_time_utc_converter}) \n",
      "    \n",
      "    def get_sw_faults_(self):\n",
      "        return {faultdata['id']: faultdata for faultdata in self.pmudata.events() if faultdata['sw_annotation'] in self.valid_faults}\n",
      "    \n",
      "    def get_xl_faults_(self):\n",
      "        return {faultdata['id']:faultdata for faultdata in self.pmudata.events() if faultdata['xl_annotation'] in self.valid_faults}\n",
      "    \n",
      "    def get_normal_minutes_(self):\n",
      "        return [x for x in self.pmudata.normalminutes().atoms()]\n",
      "    \n",
      "    def get_smag_(self):\n",
      "        phased_signals,_ = signals.cluster_phase_signals(signals.SIGNALS_2013)\n",
      "        phased_angles = {key: phased_signals[key] for key in phased_signals \n",
      "                             if len(phased_signals[key]) > 0 and key.endswith('Voltage Ang')}\n",
      "        phased_mags = {key: phased_signals[key] for key in phased_signals \n",
      "                             if len(phased_signals[key]) > 0 and key.endswith('Voltage Mag')}\n",
      "        return phased_mags.keys(), phased_mags, phased_angles\n",
      "    \n",
      "    def get_smag_2014_(self):\n",
      "        columns = signals.SIGNALS_2014\n",
      "        _PHASEANGRE = re.compile('\\|V\\|[ABC]\\|ANG')\n",
      "        _PHASEMAGRE = re.compile('\\|V\\|[ABC]\\|MAG')\n",
      "        _GENMAGRE = re.compile('\\|VP\\|\\|MAG')\n",
      "        _GENANGRE = re.compile('\\|VP\\|\\|ANG')\n",
      "\n",
      "        phased_angles = {key: [column for column in columns if _GENANGRE.sub('', key) == _PHASEANGRE.sub('', column)] \n",
      "                                 for key in [x for x in columns if _GENANGRE.search(x)] }\n",
      "        phased_mags = {key: [column for column in columns if _GENMAGRE.sub('', key) == _PHASEMAGRE.sub('', column)] \n",
      "                                 for key in [x for x in columns if _GENMAGRE.search(x)] }\n",
      "        return phased_mags.keys(), phased_mags, phased_angles\n",
      "\n",
      "    def get_smag_(self):\n",
      "        phased_signals,_ = signals.cluster_phase_signals(signals.SIGNALS_2013)\n",
      "        phased_angles = {key: phased_signals[key] for key in phased_signals \n",
      "                             if len(phased_signals[key]) > 0 and key.endswith('Voltage Ang')}\n",
      "        phased_mags = {key: phased_signals[key] for key in phased_signals \n",
      "                             if len(phased_signals[key]) > 0 and key.endswith('Voltage Mag')}\n",
      "        return phased_mags.keys(), phased_mags, phased_angles\n",
      "    \n",
      "    def get_signal_id_by_(self, depricate):\n",
      "        return [idx for idx, signal in enumerate(signals.SIGNALS_2013) if depricate(signal)]\n",
      "\n",
      "    def get_neigbour_signals_(self, signal, n_neigbour=2, predicator=None):\n",
      "        neigbour_signals = []\n",
      "        signals, _, _ = self.get_smag()\n",
      "        neigbour_bus = []\n",
      "        neigbour_sites = []\n",
      "        \n",
      "        for x in signals:\n",
      "            if x == signal or self.sitetranslator(x) == self.sitetranslator(signal): continue\n",
      "            if self.sitetranslator(x) in neigbour_sites: continue\n",
      "            ed, hop = self.get_distance_(x, signal, 0), self.get_distance_(x, signal, 1)       \n",
      "            if (not predicator(x, ed, hop)): continue\n",
      "            neigbour_bus.append((hop, x))\n",
      "            neigbour_sites.append(self.sitetranslator(x))\n",
      "            if len(neigbour_bus) == n_neigbour: break\n",
      "        \n",
      "        return sorted(neigbour_bus)\n",
      "    \n",
      "    def repopulate_result(self, df, minute):\n",
      "        assert df.shape[0] == 3600, 'Wrong data length'\n",
      "        data, ss = self.scanned_data[minute], self.scanned_ss[minute]\n",
      "        classified_data = {}\n",
      "        _, smags, _ = self.get_smag()\n",
      "        \n",
      "        for site in data:\n",
      "            if data[site][1] is None or len(data[site][1]) == 0: \n",
      "                #classified_data[site] = None\n",
      "                continue\n",
      "            start_i = 60*60 - data[site][0]\n",
      "            # poulate results\n",
      "            cycles = [3.0 for i in range(60*60)]\n",
      "            for c, t_p in data[site][1]:\n",
      "                cycles[c-start_i] = t_p\n",
      "            # poulate steady_sate\n",
      "            recomputed_ss = []\n",
      "            signaldata, moving_ss = ss[site][0], ss[site][1]\n",
      "            datapoints = common.convert_2014_smag(df[smags[site]].values)\n",
      "            for idx, datapoint in enumerate(datapoints):\n",
      "                if cycles[idx] == -1: \n",
      "                    recomputed_ss.append(None)\n",
      "                    continue\n",
      "                recomputed_ss.append(tuple([np.mean(signal) for signal in signaldata]))\n",
      "                if (moving_ss[idx - start_i]):\n",
      "                    signaldata = tuple([np.append(np.delete(signaldata[i], 0), datapoint[i]) for i in range(len(signaldata))]) \n",
      "            #compute delta_v\n",
      "            #delta_v = [common.compute_delta_v(dp, ds) for (idx, dp), ds in zip(datapoints.iterrows(), recomputed_ss)]\n",
      "           \n",
      "            classified_data[site] = cycles, start_i, recomputed_ss\n",
      "        return classified_data\n",
      "    \n",
      "    \"\"\"\n",
      "    def repopulate_n_results(self, minute, n):\n",
      "        dfs = common.chunk_date_range(minute, 0, n*60)\n",
      "        classified_data = {}\n",
      "        \n",
      "        for minute in dfs:\n",
      "            sub_result = self.repopulate_result(minute)\n",
      "            for site in sub_result:\n",
      "                if site not in classified_data:\n",
      "                    classified_data[site] = sub_result[site]\n",
      "                else:\n",
      "                    classified_data[site] = classified_data[site][0] + sub_result[site][0],classified_data[site][1], classified_data[site][2] + sub_result[site][2]\n",
      "        return classified_data\n",
      "    \"\"\"\n",
      "    \n",
      "    def print_fault_cycles(self, at_least_n_pmus_at_fault=3, at_most_n_pmus_at_fault=100, minutes = None):\n",
      "        if minutes is None: minutes = self.scanned_data.keys()\n",
      "        \n",
      "        for idx, minute in enumerate(sorted(self.scanned_data)):           \n",
      "            if not minute in minutes: continue    \n",
      "            print idx, minute\n",
      "            \n",
      "            cycles = [[] for x in range(60*60)]\n",
      "            for site in self.scanned_data[minute]:\n",
      "                if self.scanned_data[minute][site][1] is None: continue\n",
      "                for c, t_p in self.scanned_data[minute][site][1]:\n",
      "                    if np.isnan(t_p) or t_p == -1: continue # steady state window \n",
      "                    cycles[c].append(site[:4])\n",
      "            cycles = [set(c) for c in cycles]\n",
      "            print '\\tMax PMUs sees fault', max([len(c) for c in cycles])\n",
      "            for i, c in enumerate(cycles): \n",
      "                if at_least_n_pmus_at_fault <= len(c) <= at_most_n_pmus_at_fault:\n",
      "                    print '\\t', i, 'n_PMUs sees fault', c, 'at seconds', i/60\n",
      "    \n",
      "    def load_scanned_info_from(self, directory, prefix='nofault', signals=None):\n",
      "        self.scanned_data = {}\n",
      "        for filename in sorted(filter(lambda x: isfile(join(directory, x)) and x.startswith(prefix), listdir(directory))):\n",
      "            subdata = common.loadpickle(join(directory, filename))\n",
      "            for minute in subdata:\n",
      "                if minute not in self.scanned_data: \n",
      "                    self.scanned_data[minute] = {}\n",
      "                for site in subdata[minute]:\n",
      "                    if not(signals is None or site in signals): continue\n",
      "                    self.scanned_data[minute][site] = subdata[minute][site]\n",
      "    \n",
      "    def load_scanned_ss_info_from(self, directory, prefix='nofault_ss'):\n",
      "        self.scanned_ss = {}\n",
      "        for filename in sorted(filter(lambda x: isfile(join(directory, x)) and x.startswith(prefix), listdir(directory))):\n",
      "            subdata = common.loadpickle(join(directory, filename))\n",
      "            for minute in subdata:\n",
      "                if minute not in self.scanned_ss: \n",
      "                    self.scanned_ss[minute] = {}\n",
      "                for site in subdata[minute]:\n",
      "                    self.scanned_ss[minute][site] = subdata[minute][site]\n",
      "    \n",
      "    \"\"\"\n",
      "    def print_scan_detail(self, minutes = None):\n",
      "        print 'Number of minutes:', len(self.scanned_data)\n",
      "        total_scan = 0\n",
      "        total_scan_classification = 0\n",
      "        for minute in sorted(self.scanned_data):\n",
      "            if not (minutes is None or minute in minutes): continue\n",
      "            print minute\n",
      "            \n",
      "            for site in self.scanned_data[minute]:\n",
      "                if self.scanned_data[minute][site][0] is None: continue\n",
      "                n_classified_cycles, inclassified_cycles = self.scanned_data[minute][site]\n",
      "                if len(inclassified_cycles) > 0:\n",
      "                    print '\\t', site, 'number of faulted cycles', len(inclassified_cycles) \n",
      "    \"\"\"\n",
      "    \n",
      "    #PAML papers \n",
      "    #data from common.get_fn_normal_minutes()[0]\n",
      "    def get_training_and_testing_normal_data_key(self):\n",
      "        arr = [(-22, 23), (-21, 20), (-21, 21), (-21, 22), (-21, 23), (-20, 30), (-20, 31), (-20, 32), (-20, 33), (-19, 10), (-19, 11), (-19, 12), (-19, 30), (-19, 31), (-19, 32), (-19, 33), (-18, 10), (-18, 11), (-18, 12), (-18, 13), (-17, 0), (-17, 1), (-17, 3), (-17, 10), (-17, 11), (-17, 12), (-17, 13), (-17, 30), (-17, 31), (-17, 32), (-17, 33), (-17, 50), (-17, 51), (-17, 52), (-17, 53), (-14, 30), (-14, 31), (-14, 32), (-14, 33), (-14, 50), (-14, 51), (-14, 52), (-14, 53), (-13, 10), (-13, 11), (-13, 12), (-13, 13), (-12, 20), (-12, 21), (-12, 22), (-12, 23), (-12, 50), (-12, 51), (-12, 52), (-12, 53), (-10, 10), (-10, 11), (-10, 12), (-10, 13), (-8, 30), (-8, 31), (-8, 32), (-8, 33), (-7, 30), (-7, 31), (-7, 32), (-7, 33), (-6, 30), (-6, 31), (-6, 32), (-6, 33), (-6, 50), (-6, 51), (-6, 52), (-6, 53), (-5, 31), (-5, 32), (-5, 33), (-2, 10), (-2, 11), (-2, 12), (-2, 13), (-1, 0), (-1, 1), (-1, 2), (-1, 3), (-31, 0), (-31, 1), (-31, 2), (-31, 3), (-31, 30), (-31, 31), (-31, 32), (-31, 33), (-30, 0), (-30, 1), (-30, 2), (-30, 3), (-30, 40), (-30, 41), (-30, 42), (-30, 43), (-28, 10), (-28, 11), (-28, 12), (-28, 13), (-28, 40), (-28, 41), (-28, 43), (-27, 0), (-27, 1), (-27, 2), (-27, 3), (-27, 10), (-27, 11), (-27, 12), (-27, 13), (-26, 0), (-26, 1), (-26, 2), (-26, 3), (-24, 0), (-24, 1), (-24, 2), (-22, 10), (-22, 11), (-22, 12), (-22, 13), (-22, 50), (-22, 51), (-22, 52), (-22, 53), (-21, 10), (-21, 11), (-21, 12), (-21, 13), (-20, 0), (-20, 1), (-20, 2), (-20, 3), (-18, 0), (-18, 1), (-18, 2), (-18, 3), (-18, 30), (-18, 31), (-18, 32), (-18, 33), (-18, 50), (-18, 51), (-18, 52), (-18, 53), (-16, 0), (-16, 1), (-16, 2), (-16, 3), (-16, 40), (-16, 41), (-16, 42), (-16, 43), (-14, 0), (-14, 1), (-14, 3), (-14, 40), (-14, 41), (-14, 42), (-14, 43), (-13, 30), (-13, 31), (-13, 32), (-13, 33), (-11, 10), (-11, 11), (-11, 12), (-11, 13), (-11, 20), (-11, 21), (-11, 23), (-10, 40), (-10, 41), (-10, 42), (-10, 43), (-8, 40), (-8, 41), (-8, 42), (-8, 43), (-6, 0), (-6, 1), (-6, 2), (-6, 3), (-5, 10), (-5, 11), (-5, 12), (-5, 13), (-4, 0), (-4, 1)]\n",
      "        return [eid for index, eid in enumerate(arr) if index % 2 ==0] , [eid for index, eid in enumerate(arr) if index % 2 > 0]\n",
      "    \n",
      "    def get_training_and_testing_normal_data_key_random_sampling(self):\n",
      "        arr = [(-8, 31), (-7, 31), (-21, 23), (-20, 31), (-20, 33), (-19, 11), (-19, 30), (-19, 32), (-18, 10), (-18, 12), (-17, 0), (-17, 3), (-17, 11), (-17, 13), (-17, 31), (-17, 33), (-17, 51), (-17, 53), (-14, 31), (-14, 33), (-14, 51), (-14, 53), (-13, 11), (-13, 13), (-12, 21), (-12, 23), (-12, 51), (-12, 53), (-10, 11), (-10, 13), (-8, 31), (-8, 33), (-7, 31), (-7, 33), (-6, 31), (-6, 33), (-6, 51), (-6, 53), (-5, 32), (-2, 10), (-2, 12), (-1, 0), (-1, 2), (-31, 0), (-31, 2), (-31, 30), (-31, 32), (-30, 0), (-30, 2), (-30, 40), (-30, 42), (-28, 10), (-28, 12), (-28, 40), (-28, 43), (-27, 1), (-27, 3), (-27, 11), (-27, 13), (-26, 1), (-26, 3), (-24, 1), (-22, 10), (-22, 12), (-22, 50), (-22, 52), (-21, 10), (-21, 12), (-20, 0), (-20, 2), (-18, 0), (-18, 2), (-18, 30), (-18, 32), (-18, 50), (-18, 52), (-16, 0), (-16, 2), (-16, 40), (-16, 42), (-14, 0), (-14, 3), (-14, 41), (-14, 43), (-13, 31), (-13, 33), (-11, 11), (-11, 13), (-11, 21), (-10, 40), (-10, 42), (-8, 40), (-8, 42), (-6, 0), (-6, 2), (-5, 10), (-28, 42), (-14, 2), (-8, 33), (-7, 33), (-20, 30), (-20, 32), (-19, 10), (-19, 12), (-19, 31), (-19, 33), (-18, 11), (-18, 13), (-17, 1), (-17, 10), (-17, 12), (-17, 30), (-17, 32), (-17, 50), (-17, 52), (-14, 30), (-14, 32), (-14, 50), (-14, 52), (-13, 10), (-13, 12), (-12, 20), (-12, 22), (-12, 50), (-12, 52), (-10, 10), (-10, 12), (-8, 30), (-8, 32), (-7, 30), (-7, 32), (-6, 30), (-6, 32), (-6, 50), (-6, 52), (-5, 31), (-5, 33), (-2, 11), (-2, 13), (-1, 1), (-1, 3), (-31, 1), (-31, 3), (-31, 31), (-31, 33), (-30, 1), (-30, 3), (-30, 41), (-30, 43), (-28, 11), (-28, 13), (-28, 41), (-27, 0), (-27, 2), (-27, 10), (-27, 12), (-26, 0), (-26, 2), (-24, 0), (-24, 2), (-22, 11), (-22, 13), (-22, 51), (-22, 53), (-21, 11), (-21, 13), (-20, 1), (-20, 3), (-18, 1), (-18, 3), (-18, 31), (-18, 33), (-18, 51), (-18, 53), (-16, 1), (-16, 3), (-16, 41), (-16, 43), (-14, 1), (-14, 40), (-14, 42), (-13, 30), (-13, 32), (-11, 10), (-11, 12), (-11, 20), (-11, 23), (-10, 41), (-10, 43), (-8, 41), (-8, 43), (-6, 1), (-6, 3), (-5, 11), (-24, 3), (-4, 33)]\n",
      "        return [eid for index, eid in enumerate(arr) if index % 2 ==0] , [eid for index, eid in enumerate(arr) if index % 2 > 0]\n",
      "    \n",
      "    #data from common.get_fn_normal_minutes_20160101()[0]\n",
      "    def get_extra_random_key_20160101(self):\n",
      "        return 'txt/normarl_array_5_extra.pickle', [(-17, 70), (-51, 105), (-17, 73), (-44, 110), (-23, 131), (-11, 75), (-24, 12), (-1, 134), (-51, 15), (-6, 72), (-3, 105), (-15, 120), (-51, 13), (-3, 103), (-23, 132), (-5, 44), (-41, 133), (-31, 112), (-5, 15), (-15, 125), (-49, 135), (-47, 45), (-31, 113), (-49, 130), (-6, 71), (-39, 112), (-44, 112), (-41, 130), (-16, 134), (-17, 74), (-11, 70), (-37, 24), (-2, 45), (-7, 15), (-2, 2), (-25, 33), (-20, 73), (-21, 65), (-7, 13), (-7, 61), (-7, 113), (-7, 14), (-24, 123), (-25, 91), (-7, 71), (-33, 125), (-9, 23), (-18, 80), (-25, 92), (-15, 64), (-7, 114), (-47, 124), (-37, 22), (-49, 25), (-33, 10), (-12, 20), (-45, 10), (-25, 95), (-33, 120), (-33, 122), (-48, 72), (-48, 71), (-12, 21), (-32, 82), (-48, 90), (-33, 35), (-24, 105), (-7, 91), (-18, 11), (-42, 115), (-18, 21), (-18, 120), (-24, 85), (-23, 120), (-42, 51), (-46, 110), (-54, 130), (-7, 94), (-13, 2), (-24, 101), (-44, 63), (-22, 52), (-15, 114), (-44, 65), (-39, 73), (-24, 44), (-8, 101), (-26, 91), (-17, 55), (-22, 63), (-45, 21), (-42, 40), (-26, 74), (-39, 52), (-17, 54), (-29, 133), (-51, 34), (-22, 62), (-15, 10), (-16, 5), (-5, 4), (-15, 93), (-30, 50), (-48, 113), (-2, 21), (-26, 94), (-44, 70), (-47, 50), (-5, 125), (-43, 45), (-20, 135), (-17, 50), (-27, 123), (-20, 82), (-52, 20), (-8, 51), (-51, 21), (-47, 31), (-10, 80), (-42, 32), (-8, 54), (-8, 81), (-17, 133), (-11, 93), (-52, 41), (-6, 12), (-22, 125), (-40, 120), (-9, 52), (-28, 94), (-40, 110), (-25, 5), (-5, 75), (-9, 65), (-49, 110), (-38, 134), (-9, 130), (-25, 82), (-6, 15), (-35, 32), (-9, 133), (-34, 114), (-11, 104), (-27, 24), (-23, 44), (-18, 62), (-18, 63), (-26, 5), (-11, 81), (-36, 83), (-27, 23), (-34, 113), (-33, 100)]\n",
      "    \n",
      "    def get_training_normal_data_key_20160101(self):\n",
      "        return 'txt/normal_array_5.pickle', [(-27, 1), (-22, 102), (-6, 122), (-31, 80), (-52, 40), (-49, 131), (-33, 121), (-4, 120), (-30, 41), (-10, 10), (-2, 100), (-17, 61), (-10, 12), (-4, 0), (-49, 82), (-16, 123), (-55, 52), (-38, 91), (-30, 54), (-8, 91), (-39, 131), (-14, 120), (-15, 33), (-12, 54), (-30, 44), (-17, 12), (-34, 2), (-32, 11), (-4, 22), (-29, 31), (-54, 104), (-55, 24), (-54, 15), (-8, 63), (-26, 115), (-25, 20), (-33, 95), (-14, 21), (-8, 92), (-28, 103), (-8, 71), (-25, 22), (-15, 0), (-17, 15), (-45, 40), (-8, 93), (-26, 61), (-7, 133), (-52, 13), (-12, 35), (-40, 22), (-31, 35), (-15, 3), (-3, 31), (-14, 124), (-8, 72), (-5, 3), (-26, 112), (-55, 51), (-22, 4), (-5, 5), (-14, 25), (-22, 112), (-32, 10), (-20, 1), (-5, 31), (-34, 52), (-31, 131), (-51, 112), (-32, 113), (-44, 51), (-47, 13), (-11, 4), (-45, 14), (-2, 22), (-18, 15), (-49, 52), (-35, 92), (-32, 51), (-18, 13), (-54, 45), (-47, 12), (-2, 130), (-15, 73), (-39, 2), (-53, 84), (-9, 25), (-31, 40), (-30, 103), (-12, 104), (-2, 75), (-30, 32), (-18, 53), (-18, 134), (-41, 82), (-41, 30), (-8, 44), (-47, 94), (-50, 41), (-3, 63), (-47, 74), (-24, 135), (-17, 95), (-8, 40), (-18, 51), (-37, 131), (-13, 60), (-7, 81), (-15, 82), (-18, 133), (-32, 70), (-35, 3), (-29, 80), (-14, 105), (-45, 32), (-50, 42), (-41, 101), (-17, 41), (-22, 121), (-38, 121), (-21, 10), (-24, 24), (-2, 54), (-25, 81), (-16, 22), (-37, 63), (-14, 13), (-45, 82)]\n",
      "    \n",
      "    def get_testing_normal_data_key_20160101(self):\n",
      "        return 'txt/normal_array_5.pickle', [(-39, 74), (-23, 80), (-13, 1), (0, 94), (-40, 23), (-3, 0), (-37, 81), (-25, 25), (-13, 71), (-17, 11), (0, 113), (-11, 24), (0, 112), (-15, 35), (-40, 55), (-25, 15), (-38, 92), (-30, 53), (-55, 60), (-8, 25), (-48, 15), (-4, 125), (-49, 133), (-21, 124), (-8, 70), (-49, 83), (-11, 43), (-29, 70), (-22, 82), (-12, 2), (-28, 22), (-31, 100), (-3, 4), (-4, 30), (-52, 45), (-51, 100), (-9, 123), (-29, 34), (-42, 33), (-6, 83), (-9, 120), (-29, 32), (-47, 134), (-37, 40), (-26, 114), (-3, 12), (-13, 75), (-50, 5), (-47, 61), (-49, 80), (-8, 121), (-7, 134), (-55, 20), (-47, 60), (-20, 5), (-34, 0), (-12, 1), (-17, 10), (-6, 82), (-22, 83), (-33, 91), (-8, 94), (-39, 135), (-20, 70), (-25, 13), (-14, 22), (-49, 62), (-46, 23), (-33, 104), (-17, 30), (-34, 81), (-10, 134), (-47, 1), (-28, 125), (-41, 100), (-32, 134), (-14, 134), (-50, 43), (-46, 24), (-31, 41), (-33, 101), (-23, 110), (-1, 114), (-44, 121), (-45, 3), (-9, 114), (-25, 44), (-16, 24), (-32, 131), (-24, 75), (-34, 94), (-37, 20), (-2, 24), (-53, 83), (-23, 102), (-46, 33), (-31, 61), (-34, 84), (-40, 124), (-36, 23), (-50, 80), (0, 135), (-51, 11), (-49, 32), (-33, 105), (-33, 103), (-24, 74), (-54, 40), (-45, 4), (0, 33), (-51, 115), (-7, 80), (-25, 55), (-36, 24), (-54, 42), (-47, 2), (-14, 12), (-7, 32), (-2, 53), (-32, 41), (-2, 70), (-9, 112), (-3, 73), (-18, 123), (-36, 54), (-16, 62), (-3, 72), (-33, 114)]\n",
      "    \n",
      "    def get_training_random_sampling_data_key_20160101(self):\n",
      "        return 'txt/normal_array_5.pickle', [(-33, 13), (-8, 100), (-2, 35), (-53, 22), (-30, 41), (-10, 10), (-2, 100), (-17, 61), (-10, 12), (-4, 0), (-49, 82), (-16, 123), (-55, 52), (-38, 91), (-30, 54), (-8, 91), (-39, 131), (-14, 120), (-15, 33), (-12, 54), (-30, 44), (-17, 12), (-34, 2), (-32, 11), (-4, 22), (-29, 31), (-54, 104), (-55, 24), (-54, 15), (-8, 63), (-26, 115), (-25, 20), (-33, 95), (-14, 21), (-8, 92), (-28, 103), (-8, 71), (-25, 22), (-15, 0), (-17, 15), (-45, 40), (-8, 93), (-26, 61), (-7, 133), (-52, 13), (-12, 35), (-40, 22), (-31, 35), (-15, 3), (-3, 31), (-14, 124), (-8, 72), (-5, 3), (-26, 112), (-55, 51), (-22, 4), (-5, 5), (-14, 25), (-22, 112), (-32, 10), (-20, 1), (-5, 31), (-40, 20), (-32, 60), (-22, 3), (-12, 55), (-45, 135), (-50, 43), (-46, 24), (-31, 41), (-33, 101), (-23, 110), (-1, 114), (-44, 121), (-45, 3), (-9, 114), (-25, 44), (-16, 24), (-32, 131), (-24, 75), (-34, 94), (-37, 20), (-2, 24), (-53, 83), (-23, 102), (-46, 33), (-31, 61), (-34, 84), (-40, 124), (-36, 23), (-50, 80), (0, 135), (-51, 11), (-49, 32), (-33, 105), (-33, 103), (-24, 74), (-54, 40), (-45, 4), (0, 33), (-51, 115), (-7, 80), (-25, 55), (-36, 24), (-54, 42), (-47, 2), (-14, 12), (-7, 32), (-2, 53), (-32, 41), (-2, 70), (-9, 112), (-3, 73), (-18, 123), (-36, 54), (-16, 62), (-3, 72), (-33, 114), (-48, 44), (-47, 5), (-39, 0), (-34, 95), (-32, 55), (-21, 115), (-16, 60), (-10, 103), (-9, 115), (-7, 11)]\n",
      "\n",
      "    def get_testing_random_sampling_data_key_20160101(self):\n",
      "        return 'txt/normal_array_5.pickle', [(-6, 101), (-6, 135), (-1, 102), (-29, 105), (-13, 71), (-17, 11), (0, 113), (-11, 24), (0, 112), (-15, 35), (-40, 55), (-25, 15), (-38, 92), (-30, 53), (-55, 60), (-8, 25), (-48, 15), (-4, 125), (-49, 133), (-21, 124), (-8, 70), (-49, 83), (-11, 43), (-29, 70), (-22, 82), (-12, 2), (-28, 22), (-31, 100), (-3, 4), (-4, 30), (-52, 45), (-51, 100), (-9, 123), (-29, 34), (-42, 33), (-6, 83), (-9, 120), (-29, 32), (-47, 134), (-37, 40), (-26, 114), (-3, 12), (-13, 75), (-50, 5), (-47, 61), (-49, 80), (-8, 121), (-7, 134), (-55, 20), (-47, 60), (-20, 5), (-34, 0), (-12, 1), (-17, 10), (-6, 82), (-22, 83), (-33, 91), (-8, 94), (-39, 135), (-20, 70), (-25, 13), (-14, 22), (-32, 14), (-22, 1), (-18, 35), (-9, 122), (-12, 122), (-32, 51), (-18, 13), (-54, 45), (-47, 12), (-2, 130), (-15, 73), (-39, 2), (-53, 84), (-9, 25), (-31, 40), (-30, 103), (-12, 104), (-2, 75), (-30, 32), (-18, 53), (-18, 134), (-41, 82), (-41, 30), (-8, 44), (-47, 94), (-50, 41), (-3, 63), (-47, 74), (-24, 135), (-17, 95), (-8, 40), (-18, 51), (-37, 131), (-13, 60), (-7, 81), (-15, 82), (-18, 133), (-32, 70), (-35, 3), (-29, 80), (-14, 105), (-45, 32), (-50, 42), (-41, 101), (-17, 41), (-22, 121), (-38, 121), (-21, 10), (-24, 24), (-2, 54), (-25, 81), (-16, 22), (-37, 63), (-14, 13), (-45, 82), (-49, 53), (-48, 115), (-45, 111), (-36, 55), (-32, 45), (-21, 35), (-18, 52), (-12, 95), (-9, 22), (-7, 5), (-2, 64)]\n",
      "    \n",
      "    #obsolete\n",
      "    def get_training_and_testing_normal_data_key_20160101(self):\n",
      "        arr = [(-27, 1), (-39, 74), (-22, 102), (-23, 80), (-6, 122), (-13, 1), (-31, 80), (0, 94), (-52, 40), (-40, 23), (-49, 131), (-3, 0), (-33, 121), (-37, 81), (-4, 120), (-25, 25), (-30, 41), (-13, 71), (-10, 10), (-17, 11), (-2, 100), (0, 113), (-17, 61), (-11, 24), (-10, 12), (0, 112), (-4, 0), (-15, 35), (-49, 82), (-40, 55), (-16, 123), (-25, 15), (-55, 52), (-38, 92), (-38, 91), (-30, 53), (-30, 54), (-55, 60), (-8, 91), (-8, 25), (-39, 131), (-48, 15), (-14, 120), (-4, 125), (-15, 33), (-49, 133), (-12, 54), (-21, 124), (-30, 44), (-8, 70), (-17, 12), (-49, 83), (-34, 2), (-11, 43), (-32, 11), (-29, 70), (-4, 22), (-22, 82), (-29, 31), (-12, 2), (-54, 104), (-28, 22), (-55, 24), (-31, 100), (-54, 15), (-3, 4), (-8, 63), (-4, 30), (-26, 115), (-52, 45), (-25, 20), (-51, 100), (-33, 95), (-9, 123), (-14, 21), (-29, 34), (-8, 92), (-42, 33), (-28, 103), (-6, 83), (-8, 71), (-9, 120), (-25, 22), (-29, 32), (-15, 0), (-47, 134), (-17, 15), (-37, 40), (-45, 40), (-26, 114), (-8, 93), (-3, 12), (-26, 61), (-13, 75), (-7, 133), (-50, 5), (-52, 13), (-47, 61), (-12, 35), (-49, 80), (-40, 22), (-8, 121), (-31, 35), (-7, 134), (-15, 3), (-55, 20), (-3, 31), (-47, 60), (-14, 124), (-20, 5), (-8, 72), (-34, 0), (-5, 3), (-12, 1), (-26, 112), (-17, 10), (-55, 51), (-6, 82), (-22, 4), (-22, 83), (-5, 5), (-33, 91), (-14, 25), (-8, 94), (-22, 112), (-39, 135), (-32, 10), (-20, 70), (-20, 1), (-25, 13), (-5, 31), (-14, 22), (-34, 52), (-49, 62), (-31, 131), (-46, 23), (-51, 112), (-33, 104), (-32, 113), (-17, 30), (-44, 51), (-34, 81), (-47, 13), (-10, 134), (-11, 4), (-47, 1), (-45, 14), (-28, 125), (-2, 22), (-41, 100), (-18, 15), (-32, 134), (-49, 52), (-14, 134), (-35, 92), (-50, 43), (-32, 51), (-46, 24), (-18, 13), (-31, 41), (-54, 45), (-33, 101), (-47, 12), (-23, 110), (-2, 130), (-1, 114), (-15, 73), (-44, 121), (-39, 2), (-45, 3), (-53, 84), (-9, 114), (-9, 25), (-25, 44), (-31, 40), (-16, 24), (-30, 103), (-32, 131), (-12, 104), (-24, 75), (-2, 75), (-34, 94), (-30, 32), (-37, 20), (-18, 53), (-2, 24), (-18, 134), (-53, 83), (-41, 82), (-23, 102), (-41, 30), (-46, 33), (-8, 44), (-31, 61), (-47, 94), (-34, 84), (-50, 41), (-40, 124), (-3, 63), (-36, 23), (-47, 74), (-50, 80), (-24, 135), (0, 135), (-17, 95), (-51, 11), (-8, 40), (-49, 32), (-18, 51), (-33, 105), (-37, 131), (-33, 103), (-13, 60), (-24, 74), (-7, 81), (-54, 40), (-15, 82), (-45, 4), (-18, 133), (0, 33), (-32, 70), (-51, 115), (-35, 3), (-7, 80), (-29, 80), (-25, 55), (-14, 105), (-36, 24), (-45, 32), (-54, 42), (-50, 42), (-47, 2), (-41, 101), (-14, 12), (-17, 41), (-7, 32), (-22, 121), (-2, 53), (-38, 121), (-32, 41), (-21, 10), (-2, 70), (-24, 24), (-9, 112), (-2, 54), (-3, 73), (-25, 81), (-18, 123), (-16, 22), (-36, 54), (-37, 63), (-16, 62), (-14, 13), (-3, 72), (-45, 82), (-33, 114)]\n",
      "        return [eid for index, eid in enumerate(arr) if index % 2 ==0] , [eid for index, eid in enumerate(arr) if index % 2 > 0]\n",
      "    \n",
      "    #obsolete\n",
      "    def get_training_and_testing_normal_data_key_random_sampling_20160101(self):\n",
      "        arr = [(-33, 13), (-6, 101), (-8, 100), (-6, 135), (-2, 35), (-1, 102), (-53, 22), (-29, 105), (-30, 41), (-13, 71), (-10, 10), (-17, 11), (-2, 100), (0, 113), (-17, 61), (-11, 24), (-10, 12), (0, 112), (-4, 0), (-15, 35), (-49, 82), (-40, 55), (-16, 123), (-25, 15), (-55, 52), (-38, 92), (-38, 91), (-30, 53), (-30, 54), (-55, 60), (-8, 91), (-8, 25), (-39, 131), (-48, 15), (-14, 120), (-4, 125), (-15, 33), (-49, 133), (-12, 54), (-21, 124), (-30, 44), (-8, 70), (-17, 12), (-49, 83), (-34, 2), (-11, 43), (-32, 11), (-29, 70), (-4, 22), (-22, 82), (-29, 31), (-12, 2), (-54, 104), (-28, 22), (-55, 24), (-31, 100), (-54, 15), (-3, 4), (-8, 63), (-4, 30), (-26, 115), (-52, 45), (-25, 20), (-51, 100), (-33, 95), (-9, 123), (-14, 21), (-29, 34), (-8, 92), (-42, 33), (-28, 103), (-6, 83), (-8, 71), (-9, 120), (-25, 22), (-29, 32), (-15, 0), (-47, 134), (-17, 15), (-37, 40), (-45, 40), (-26, 114), (-8, 93), (-3, 12), (-26, 61), (-13, 75), (-7, 133), (-50, 5), (-52, 13), (-47, 61), (-12, 35), (-49, 80), (-40, 22), (-8, 121), (-31, 35), (-7, 134), (-15, 3), (-55, 20), (-3, 31), (-47, 60), (-14, 124), (-20, 5), (-8, 72), (-34, 0), (-5, 3), (-12, 1), (-26, 112), (-17, 10), (-55, 51), (-6, 82), (-22, 4), (-22, 83), (-5, 5), (-33, 91), (-14, 25), (-8, 94), (-22, 112), (-39, 135), (-32, 10), (-20, 70), (-20, 1), (-25, 13), (-5, 31), (-14, 22), (-40, 20), (-32, 14), (-32, 60), (-22, 1), (-22, 3), (-18, 35), (-12, 55), (-9, 122), (-45, 135), (-12, 122), (-50, 43), (-32, 51), (-46, 24), (-18, 13), (-31, 41), (-54, 45), (-33, 101), (-47, 12), (-23, 110), (-2, 130), (-1, 114), (-15, 73), (-44, 121), (-39, 2), (-45, 3), (-53, 84), (-9, 114), (-9, 25), (-25, 44), (-31, 40), (-16, 24), (-30, 103), (-32, 131), (-12, 104), (-24, 75), (-2, 75), (-34, 94), (-30, 32), (-37, 20), (-18, 53), (-2, 24), (-18, 134), (-53, 83), (-41, 82), (-23, 102), (-41, 30), (-46, 33), (-8, 44), (-31, 61), (-47, 94), (-34, 84), (-50, 41), (-40, 124), (-3, 63), (-36, 23), (-47, 74), (-50, 80), (-24, 135), (0, 135), (-17, 95), (-51, 11), (-8, 40), (-49, 32), (-18, 51), (-33, 105), (-37, 131), (-33, 103), (-13, 60), (-24, 74), (-7, 81), (-54, 40), (-15, 82), (-45, 4), (-18, 133), (0, 33), (-32, 70), (-51, 115), (-35, 3), (-7, 80), (-29, 80), (-25, 55), (-14, 105), (-36, 24), (-45, 32), (-54, 42), (-50, 42), (-47, 2), (-41, 101), (-14, 12), (-17, 41), (-7, 32), (-22, 121), (-2, 53), (-38, 121), (-32, 41), (-21, 10), (-2, 70), (-24, 24), (-9, 112), (-2, 54), (-3, 73), (-25, 81), (-18, 123), (-16, 22), (-36, 54), (-37, 63), (-16, 62), (-14, 13), (-3, 72), (-45, 82), (-33, 114), (-49, 53), (-48, 44), (-48, 115), (-47, 5), (-45, 111), (-39, 0), (-36, 55), (-34, 95), (-32, 45), (-32, 55), (-21, 35), (-21, 115), (-18, 52), (-16, 60), (-12, 95), (-10, 103), (-9, 22), (-9, 115), (-7, 5), (-7, 11), (-2, 64)]\n",
      "        return [eid for index, eid in enumerate(arr) if index % 2 ==0] , [eid for index, eid in enumerate(arr) if index % 2 > 0]\n",
      "    \n",
      "    #the function will return a list of data point for xl original classification and cascade function\n",
      "    def load_faulted_array(self, events_fn):\n",
      "        faulted_array = []\n",
      "        id_events, id_phaseAngles, id_phaseAngles_ss = common.loadpickle(events_fn)\n",
      "        for faultdata in self.pmudata.events():\n",
      "            faultid = faultdata['id']\n",
      "            if faultid not in id_events: continue\n",
      "            if faultid not in id_phaseAngles_ss or len(id_phaseAngles_ss[faultid]) ==  0: \n",
      "                print 'skip', faultid \n",
      "                continue\n",
      "\n",
      "            fault_comment = faultdata['sw_annotation']\n",
      "\n",
      "            event0 = id_events[faultid][0]\n",
      "            classification0 = classifyfault_v1.classify_event(event0, quiet=True)\n",
      "\n",
      "            if (faultdata['sw_annotation'] not in self.valid_faults): \n",
      "                print 'skip', faultid\n",
      "                continue\n",
      "\n",
      "            #print 'add', faultid\n",
      "            for i in range(len(id_phaseAngles_ss[faultid])):\n",
      "                event = id_events[faultid][i]\n",
      "                angles = id_phaseAngles[faultid][i]\n",
      "                angles_ss = id_phaseAngles_ss[faultid][i]\n",
      "\n",
      "                faulted_array.append((faultid, event.signal, event.v_sag_a, event.v_sag_b, event.v_sag_c, event.dv_t(),\n",
      "                                      event.v_sag_a * event.v_ss_a, event.v_sag_b * event.v_ss_b, event.v_sag_c * event.v_ss_c,\n",
      "                                      angles[0], angles[1], angles[2], angles_ss[0], angles_ss[1], angles_ss[2], i == 0,\n",
      "                                      faultdata['sw_annotation'], classifyfault_v1.classify_event(event, quiet=True)))\n",
      "        return faulted_array\n",
      "    \n",
      "    #for IEEE papers\n",
      "    def get_data_points_for_v_sags_classifier(self):\n",
      "        _, _, normal_data = common.get_fn_normal_minutes()\n",
      "        sag_mode_fn1, sag_mode_fn2 = common.get_fn_sag_mode()\n",
      "        \n",
      "        sag_mode_array_1, sag_mode_array_2 = self.load_faulted_array(sag_mode_fn1), self.load_faulted_array(sag_mode_fn2)\n",
      "        normal_array_1, normal_array_2 = common.loadpickle(normal_data[0]), common.loadpickle(normal_data[1])\n",
      "        return sag_mode_array_1 + normal_array_1, sag_mode_array_2 + normal_array_2\n",
      "        \n",
      "    def get_data_points_for_rule_based_classifier(self):\n",
      "        _, _, normal_data = common.get_fn_normal_minutes()\n",
      "        fault_fn1, fault_fn2 = common.get_fn_new_xl_features()\n",
      "        \n",
      "        fault_array_1, fault_array_2 = common.loadpickle(fault_fn1), common.loadpickle(fault_fn2)\n",
      "        normal_array_1, normal_array_2 = common.loadpickle(normal_data[0]), common.loadpickle(normal_data[1])\n",
      "        return fault_array_1 + normal_array_1, fault_array_2 + normal_array_2\n",
      "    \n",
      "    def get_data_points_sag_mode_for_rule_based_classifier(self):\n",
      "        _, _, normal_data = common.get_fn_normal_minutes()\n",
      "        fault_fn1, fault_fn2 = common.get_fn_new_xl_features_sag_mode()\n",
      "        \n",
      "        fault_array_1, fault_array_2 = common.loadpickle(fault_fn1), common.loadpickle(fault_fn2)\n",
      "        normal_array_1, normal_array_2 = common.loadpickle(normal_data[0]), common.loadpickle(normal_data[1])\n",
      "        return fault_array_1 + normal_array_1, fault_array_2 + normal_array_2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting datafiles.py\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile learning.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.pyplot as pyplot\n",
      "from bpati import signals\n",
      "from libs.arff import Dataset\n",
      "from sklearn import cross_validation, datasets\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import *\n",
      "from sklearn.cross_validation import train_test_split, StratifiedKFold\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.learning_curve import learning_curve\n",
      "from numpy import array\n",
      "\n",
      "default_range_C = np.linspace(0.01, 20, 30)\n",
      "default_range_variable = np.linspace(0.01, 20, 30)\n",
      "coef0 = 1\n",
      "\n",
      "svc_tuned_parameters = []\n",
      "svc_tuned_parameters.append({'kernel': ['linear'], 'C': default_range_C})\n",
      "svc_tuned_parameters.append({'kernel': ['poly'], 'degree': default_range_variable, 'C': default_range_C, 'coef0': [0.5, 1, 1.5]})\n",
      "svc_tuned_parameters.append({'kernel': ['rbf'], 'gamma': default_range_variable, 'C': default_range_C, 'coef0': [0.5, 1, 1.5]})\n",
      "\n",
      "def sensitivity_scores(cfm):\n",
      "    sensitivity = (cfm[0][0]*1./(cfm[0][0] + cfm[0][1]))\n",
      "    specificity = (cfm[1][1]*1./(cfm[1][1] + cfm[1][0]))\n",
      "    precision = (cfm[0][0]*1./(cfm[0][0] + cfm[1][0]))\n",
      "    ppv = (cfm[1][1]*1./(cfm[1][1] + cfm[0][1]))\n",
      "    return sensitivity, specificity, precision, ppv\n",
      "  \n",
      "def cfm_binary_outcome(y_true, y_predict, pos_label=1):\n",
      "    y_true = convert_to_binary_outcome(y_true, pos_label)\n",
      "    y_predict = convert_to_binary_outcome(y_predict, pos_label)\n",
      "    return confusion_matrix(y_true, y_predict, labels=[1, 0])\n",
      "\n",
      "def convert_to_binary_outcome(outcomes, pos_label):\n",
      "    return [1 if x == 3.0 else 0 for x in array(outcomes).ravel()]\n",
      "\n",
      "def specificity(cfm):\n",
      "    return (cfm[1][1]*1./(cfm[1][1] + cfm[1][0]))\n",
      "\n",
      "def fpr(cfm):\n",
      "    return 1 - specificity(cfm)\n",
      "\n",
      "def fnr(cfm):\n",
      "    return (cfm[0][1] * 1.0) / (cfm[0][1] + cfm[0][0])\n",
      "\n",
      "def scores(y_true, y_predict):\n",
      "    cfm = confusion_matrix(y_true, y_predict)\n",
      "    if cfm.shape == (2, 2): return sensitivity_scores(cfm)\n",
      "    return None\n",
      "\n",
      "def copy_SVC(estimator):\n",
      "    return SVC(kernel=estimator.kernel, gamma=estimator.gamma, C=estimator.C, coef0=estimator.coef0, degree=estimator.degree)\n",
      "\n",
      "def hamming_scoring(estimator, X, y):\n",
      "    y_pred = estimator.predict(X)\n",
      "    return hamming_loss(y, y_pred)\n",
      "\n",
      "def predict_table(estimator, X, y):\n",
      "    y_pred = estimator.predict(X)\n",
      "    return [y, y_pred]\n",
      "\n",
      "def plot_between(plt, x, scores, color, label):\n",
      "    test_scores_mean = np.mean(scores, axis=1)\n",
      "    test_scores_std = np.std(scores, axis=1)\n",
      "    \n",
      "    plt.fill_between(x, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=color)\n",
      "    plt.plot(x, test_scores_mean, 'o-', color=color, label=label)\n",
      "\n",
      "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
      "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
      "    \"\"\"\n",
      "    Generate a simple plot of the test and traning learning curve.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "        An object of that type which is cloned for each validation.\n",
      "\n",
      "    title : string\n",
      "        Title for the chart.\n",
      "\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        Training vector, where n_samples is the number of samples and\n",
      "        n_features is the number of features.\n",
      "\n",
      "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
      "        Target relative to X for classification or regression;\n",
      "        None for unsupervised learning.\n",
      "\n",
      "    ylim : tuple, shape (ymin, ymax), optional\n",
      "        Defines minimum and maximum yvalues plotted.\n",
      "\n",
      "    cv : integer, cross-validation generator, optional\n",
      "        If an integer is passed, it is the number of folds (defaults to 3).\n",
      "        Specific cross-validation objects can be passed, see\n",
      "        sklearn.cross_validation module for the list of possible objects\n",
      "\n",
      "    n_jobs : integer, optional\n",
      "        Number of jobs to run in parallel (default 1).\n",
      "    \"\"\"\n",
      "    plt.figure(figsize=(8,5))\n",
      "    plt.title(title)\n",
      "    if ylim is not None:\n",
      "        plt.ylim(*ylim)\n",
      "    plt.xlabel(\"Training examples\")\n",
      "    plt.ylabel(\"Score\")\n",
      "    train_sizes, train_scores, test_scores = learning_curve(\n",
      "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
      "\n",
      "    plot_between(plt, train_sizes, train_scores, \"r\", \"Training score\")\n",
      "    plot_between(plt, train_sizes, test_scores, \"g\", \"Cross-validation score\")\n",
      "    \n",
      "    plt.grid()\n",
      "    plt.legend(loc=\"best\")\n",
      "    #plt.ylim((0,1.1))\n",
      "    return plt\n",
      "\n",
      "def plot_diagnosing_bias_variance(X, y, X_test = None, y_test = None, coef0=coef0, gammas=None, degrees=None, C=default_range_C[0], filename=''):\n",
      "    cfms = []\n",
      "    test_scores = []\n",
      "    train_scores = []\n",
      "    separate_test_score = []\n",
      "    sensitivity = []\n",
      "    specificity = []\n",
      "    precision = []\n",
      "    ppv = []\n",
      "    SVCs = []\n",
      "    \n",
      "    if X_test is None or y_test is None:\n",
      "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
      "    else:\n",
      "        X_train, y_train = X, y\n",
      "    \n",
      "    plt.figure(0, figsize=(8,5))\n",
      "    plt.figure(1, figsize=(8,5))\n",
      "    if gammas != None:\n",
      "        variables = gammas\n",
      "        SVCs = [SVC(kernel='rbf', gamma=g, C=C, coef0=coef0) for g in gammas]\n",
      "        plt.figure(0)\n",
      "        plt.xlabel(\"Gamma\")\n",
      "        plt.figure(1)\n",
      "        plt.xlabel(\"Gamma\")\n",
      "    else:\n",
      "        variables = degrees\n",
      "        SVCs = [SVC(kernel='poly', degree=d, C=C, coef0=coef0) for d in degrees]\n",
      "        plt.figure(0)\n",
      "        plt.xlabel(\"Degree\")\n",
      "        plt.figure(1)\n",
      "        plt.xlabel(\"Degree\")\n",
      "    \n",
      "    for clf in SVCs:\n",
      "        test_scores.append(cross_validation.cross_val_score(copy_SVC(clf), X_train, y_train, cv=5, n_jobs = 4, scoring=hamming_scoring))\n",
      "                    \n",
      "        model = copy_SVC(clf).fit(X_train, y_train)\n",
      "        train_scores.append(hamming_loss(y_train, model.predict(X_train)))\n",
      "        separate_test_score.append(hamming_loss(y_test, model.predict(X_test)))\n",
      "        scrs = scores(y_test, model.predict(X_test))\n",
      "        print clf\n",
      "        print confusion_matrix(y_test, model.predict(X_test))\n",
      "        print 'Hamming loss', hamming_loss(y_test, model.predict(X_test))\n",
      "        \n",
      "        if scores is not None:\n",
      "            sensitivity.append(scrs[0])\n",
      "            specificity.append(scrs[2])\n",
      "            precision.append(scrs[1])\n",
      "            ppv.append(scrs[3])\n",
      "            print 'Sensitivity', scrs[0], \n",
      "            print 'Specificity', scrs[2],\n",
      "            print 'Precision', scrs[1],\n",
      "            print 'PPV', scrs[3]\n",
      "    \n",
      "    plt.figure(0)\n",
      "    plot_between(plt, variables, test_scores, \"g\", \"Cross-validation score\")\n",
      "    plt.plot(variables, train_scores, 'o-', color=\"r\", label=\"Training score\")\n",
      "    plt.plot(variables, separate_test_score, 'o-', color=\"b\", label=\"Testing score\")\n",
      "    plt.ylabel(\"Score\")\n",
      "    plt.grid()\n",
      "    plt.legend(loc=\"best\")\n",
      "    \n",
      "    if scores is not None: \n",
      "        plt.figure(1)\n",
      "        plt.plot(variables, sensitivity, 'o-', color=\"r\", label=\"Sensitivity score\")\n",
      "        plt.plot(variables, specificity, 'o-', color=\"b\", label=\"Specificity score\")\n",
      "        plt.plot(variables, precision, 'o-', color=\"g\", label=\"Precision score\")\n",
      "        plt.plot(variables, ppv, 'o-', color=\"black\", label=\"PPV score\")    \n",
      "        plt.ylabel(\"Score\")\n",
      "        plt.grid()\n",
      "        plt.legend(loc=\"best\")\n",
      "    \n",
      "    if filename == '': plt.show()   \n",
      "    else:\n",
      "        plt.figure(0)\n",
      "        plt.savefig('%s_hammingloss.png' % filename)\n",
      "        if scores is not None:\n",
      "            plt.figure(1)\n",
      "            plt.savefig('%s_sensitivity.png' % filename)\n",
      "    return SVCs[np.array(separate_test_score).argmin()]\n",
      "\n",
      "def plot_diagnosing_bias_variance_C(X, y, clf, X_test = None, y_test = None, C = default_range_C):\n",
      "    test_scores = []\n",
      "    train_scores = []\n",
      "    separate_test_score = []\n",
      "    \n",
      "    if X_test is None or y_test is None:\n",
      "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
      "    else:\n",
      "        X_train, y_train = X, y\n",
      "        \n",
      "    for c in C:\n",
      "        clf.C = c\n",
      "        test_scores.append(cross_validation.cross_val_score(copy_SVC(clf), X, y, cv=5, n_jobs = 4, scoring = hamming_scoring))\n",
      "        \n",
      "        model = copy_SVC(clf).fit(X_train, y_train)\n",
      "        train_scores.append(hamming_loss(y_train, model.predict(X_train)))        \n",
      "        separate_test_score.append(hamming_loss(y_test, model.predict(X_test)))\n",
      "           \n",
      "    plot_between(plt, C, test_scores, \"g\", \"Cross-validation score\")\n",
      "    plt.plot(C, train_scores, 'o-', color=\"r\", label=\"Training score\")\n",
      "    plt.plot(C, separate_test_score, 'o-', color=\"b\", label=\"Testing score\")\n",
      "        \n",
      "    plt.xlabel(\"C\")\n",
      "    plt.ylabel(\"Score\")\n",
      "    plt.title(clf)\n",
      "    plt.grid()\n",
      "    plt.legend(loc=\"best\")\n",
      "    #plt.ylim((0,1.1))\n",
      "    plt.show()\n",
      "    \n",
      "def plot_diagnosing_bias_variance_SVC_poly(X, y, X_test = None, y_test = None, coef0=coef0, degrees = default_range_variable, C = default_range_C):\n",
      "    clf_best = plot_diagnosing_bias_variance(X, y, X_test = X_test, y_test = y_test, coef0=coef0, degrees = degrees)\n",
      "    plot_diagnosing_bias_variance_C(X, y, clf_best, C)\n",
      "    \n",
      "def plot_diagnosing_bias_variance_SVC_rbf(X, y, gammas = default_range_variable, C = default_range_C):\n",
      "    clf_best = plot_diagnosing_bias_variance(X, y, gammas = gammas)\n",
      "    plot_diagnosing_bias_variance_C(X, y, clf_best, C)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting learning.py\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Experimenter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile experiment.py\n",
      "#unit testing 06_experimental/ClassifierExperiment_new.ipynb\n",
      "import matplotlib.pyplot as pyplot\n",
      "from numpy import array, isnan, nan, append, delete, mean\n",
      "from pandas import DataFrame\n",
      "from libs.arff import *\n",
      "from libs.classifiers import *\n",
      "from libs.common import *\n",
      "from libs.feature import *\n",
      "from bpati import signals\n",
      "      \n",
      "def experimental_classifier_plot(df, classifier, xline=True, filename='', xlim=None):\n",
      "    ce, results, start_i, v_ss,_,_ = classifier.scan_df(df)\n",
      "    experimental_plot(df, results, classifier.signal, classifier.smags, xline, xlim, filename=filename)\n",
      "    \n",
      "def experimental_plot(df, results, signal, smags, xline, xlim, filename=''):\n",
      "    if results == None or all([isnan(r) for r in results]): \n",
      "        print 'No signals'\n",
      "        return\n",
      "    \n",
      "    result_diff_i = []\n",
      "    for idx, _class in enumerate(results):\n",
      "        if len(result_diff_i) == 0 or idx == len(results) - 1: \n",
      "            result_diff_i.append(idx)\n",
      "            continue\n",
      "        if results[idx-1] == _class: continue\n",
      "        result_diff_i.append(idx)\n",
      "    \n",
      "    phase_a, phase_b, phase_c = smags\n",
      "\n",
      "    dt = df['DateTime'].iloc[0]\n",
      "    pyplot.figure(figsize=(12, 4), dpi=100)\n",
      "    #pyplot.subplot(211)\n",
      "    pyplot.title('%s\\nStarting At: %s\\nOrange: SLG, Yellow: LtL, Blue: 3P, Purple: N/A' % (signal, str(dt)))\n",
      "    pyplot.xlabel('Cycles')\n",
      "    pyplot.ylabel('Voltage (kV)\\nA,B,C Phases in Red,Green,Blue')\n",
      "    pyplot.plot(df.index, df[phase_a], 'g', label = 'A')\n",
      "    pyplot.plot(df.index, df[phase_b], 'r--', label = 'B')\n",
      "    pyplot.plot(df.index, df[phase_c], 'k:', label = 'C')\n",
      "    pyplot.legend(loc='best', title='Phases')\n",
      "    if xlim is not None: pyplot.xlim((xlim[0], xlim[1]))\n",
      "    \n",
      "    if xline:\n",
      "        for idx in xrng: pyplot.axvline(idx, c='r', linestyle='--')\n",
      "    \n",
      "    base = df.index[0]\n",
      "    for idx, start_span in enumerate(result_diff_i[:-1]):\n",
      "        #if start_span > 38: continue\n",
      "        if (results[start_span] == 0): color = {'color':'orange', 'alpha':.5}\n",
      "        elif (results[start_span] == 1): color = {'color':'yellow', 'alpha':.5}\n",
      "        elif (results[start_span] == 2): color = {'color':'blue', 'alpha':.5}\n",
      "        elif (np.isnan(results[start_span])): color = {'color':'purple', 'alpha':.5}\n",
      "        else: continue\n",
      "        pyplot.axvspan(base + start_span, base + result_diff_i[idx+1], **color)\n",
      "    \n",
      "    if filename != '': pyplot.savefig(filename, format='pdf', bbox_inches='tight')\n",
      "    pyplot.show()  \n",
      "    pyplot.close()\n",
      "\n",
      "class ClassifyingExperimenter(object):\n",
      "    def __init__(self, signal, smags, sangles, classifier_pickle):\n",
      "        self.signal = signal\n",
      "        self.smags = smags[signal]\n",
      "        self.is2014data = common.check_2014_data(signal)\n",
      "        \n",
      "        if not self.is2014data: self.sangles = sangles[signal.replace('Mag', 'Ang')]\n",
      "        else: self.sangles = sangles[signal.replace('MAG', 'ANG')]\n",
      "        self.nominal = signals.nominalvoltage(signal)\n",
      "\n",
      "        self.last_ss_scan_datetime = None       \n",
      "        self.signaldata = None\n",
      "        self.datapoint = None\n",
      "\n",
      "        self.first_signal_data = None\n",
      "        \n",
      "        self.classifier = EventClassifier.LoadPickle(classifier_pickle)\n",
      "        pass\n",
      "        \n",
      "    #the function return the tuple: success, normal_voltages, starting point and ending point\n",
      "    def steady_state_(self, arr):\n",
      "        extent = 10\n",
      "        assert self.signaldata is not None or len(arr) > extent, 'Scan window is too narrow'\n",
      "        \n",
      "        for starti in range(0, len(arr) - extent):\n",
      "            window = arr[starti:starti + extent]\n",
      "            #print 'check nan or window <=25', any(isnan(window)) or any(window <= 25)\n",
      "            #print 'come heh', any(isnan(window)), any(window <= 25)\n",
      "            if any(isnan(window)) or any(window <= 25): continue\n",
      "            if self.is_steady_state_(window, True): return True, starti, starti + extent\n",
      "        return False, nan, nan\n",
      "       \n",
      "    def is_steady_state_(self, voltages, edge_test):\n",
      "        sagthresh=.9\n",
      "        edgethresh=.01\n",
      "        outofservice_threshold = 0.8\n",
      "        \n",
      "        w_mean, w_min = mean(voltages), min(voltages)\n",
      "        edgemin = (1.0 - edgethresh) * w_mean\n",
      "        edgemax = (1.0 + edgethresh) * w_mean\n",
      "        return w_min > sagthresh*w_mean and w_mean >= outofservice_threshold * self.nominal and\\\n",
      "                (not edge_test or (voltages[0] > edgemin and voltages[-1] > edgemin))\n",
      "        \n",
      "    def update_first_window_(self, df, steady_states, base_idx, starti):\n",
      "        extent_data_point = 3\n",
      "        self.signaldata = {key: array(df[key].loc[v_ss[1]+base_idx:v_ss[2]+base_idx]) \n",
      "                               for key, v_ss in zip(self.smags, steady_states)}\n",
      "        self.signaldata.update({key: array(df[key].loc[v_ss[1]+base_idx:v_ss[2]+base_idx]) \n",
      "                                for key, v_ss in zip(self.sangles, steady_states)})\n",
      "        self.datapoint = {key: array(df[key].loc[starti-extent_data_point:starti]) \n",
      "                              for key in self.signaldata}\n",
      "        self.last_ss_scan_datetime = df['DateTime'][starti-1]\n",
      "        \n",
      "    def update_data_window(self, df, idx):\n",
      "        for key in self.signaldata:\n",
      "            self.signaldata[key] = append(delete(self.signaldata[key], 0), df[key][idx])\n",
      "            \n",
      "        self.last_ss_scan_datetime = df['DateTime'][idx]\n",
      "        \n",
      "    # test the scan should be within datetime of steady state\n",
      "    def is_ss_indatetime(self, dt):\n",
      "        n_cycle_from_ss = 10\n",
      "        in_out_date = self.last_ss_scan_datetime is not None and \\\n",
      "                0 <= (dt - self.last_ss_scan_datetime).total_seconds() < 0.02 * n_cycle_from_ss\n",
      "        return in_out_date\n",
      "    \n",
      "    def compute_starting_point_(self, df, base_idx):\n",
      "        if self.is_ss_indatetime(df['DateTime'][base_idx]): return base_idx\n",
      "        if any([col not in df for col in self.smags+self.sangles]): return -1\n",
      "        \n",
      "        # no scan before\n",
      "        v_ss_info = [self.steady_state_(df[s].loc[base_idx:].values) for s in self.smags]\n",
      "        if not all([v_ss[0] for v_ss in v_ss_info]): return -1\n",
      "        starti = base_idx + max([v_ss[2] for v_ss in v_ss_info])\n",
      "        self.update_first_window_(df, v_ss_info, base_idx, starti)\n",
      "        return starti\n",
      "        \n",
      "    def get_exporting_event_(self, df, idx, verbose=False):       \n",
      "        for s in self.signaldata:\n",
      "            self.datapoint[s] = append(delete(self.datapoint[s], 0), df[s][idx])\n",
      "        if verbose:\n",
      "            print '\\tDatapoint'\n",
      "            for k in self.datapoint: print '\\t\\t', k, self.datapoint[k]        \n",
      "        return ExportingEvent(self.signal, self.smags, self.sangles, self.datapoint, self.signaldata, 0)        \n",
      "    \n",
      "    # the function is a combination of logics to determine that the moment of time is appropriate to move\n",
      "    # in this experiment, I would let the movement forward if the voltage is not less than 90% steady state.\n",
      "    # the sag threshod is equal to the one used in steady_state_\n",
      "    def is_safe_to_slide_(self, predict, v_a, v_b, v_c, verbose):\n",
      "        ss_flags = [self.is_steady_state_(append(delete(self.signaldata[s], 0), v), False) for v, s in zip([v_a, v_b, v_c], self.smags)]\n",
      "        if verbose:\n",
      "            print '\\tSteady state flags:'\n",
      "            for s, f in zip(self.smags, ss_flags): print '\\t\\t', s, f  \n",
      "        # if the last outcome is noFault, or is steady_state check is ok\n",
      "        return predict == 3.0 and all(ss_flags)\n",
      "    \n",
      "    # the function is to convert 2014 -> 2013 format\n",
      "    def convert_2014_data_(self, scan_df):\n",
      "        df = {col: common.convert_2014_smag(scan_df[col].values) for col in self.smags}\n",
      "        df.update({col: common.convert_2014_angs(scan_df[col].values) for col in self.sangles})\n",
      "        df.update({'DateTime': scan_df['DateTime'].values})\n",
      "        return DataFrame(data=df, index=scan_df.index.values)\n",
      "    \n",
      "    # the function is an iteration if scan that should start at start_idx and break if ss window is out date time or until reach to end\n",
      "    def scan_df_(self, df, start_idx, verbose):       \n",
      "        results, v_ss = [], []\n",
      "        start_i = self.compute_starting_point_(df, start_idx)\n",
      "        if start_i < 0: \n",
      "            if verbose: print '\\tFailing steady state computation'\n",
      "            return -1, None, None, None, None\n",
      "        \n",
      "        if verbose: print 'Staring point: ', start_i\n",
      "\n",
      "        first_v_ss = [self.signaldata[mag] for mag in self.smags]\n",
      "        results = [-1 for x in range(start_idx, start_i)] # -1 means steady state window is error or unvailable\n",
      "        updated_ss = [False for x in range(len(results))]\n",
      "        \n",
      "        for idx in df.index[df.index >= start_i]:\n",
      "            if not self.is_ss_indatetime(df['DateTime'][idx]): break\n",
      "            \n",
      "            ss_a, ss_b, ss_c = [mean(self.signaldata[s]) for s in self.smags]                           \n",
      "            if verbose:\n",
      "                print 'At cycle', idx\n",
      "                print '\\tSteady state phase A', ss_a\n",
      "                print '\\tSteady state phase B', ss_b\n",
      "                print '\\tSteady state phase C', ss_c             \n",
      "            \n",
      "            ee = self.get_exporting_event_(df, idx, verbose)\n",
      "            if not ee.error: # all success for ss computation\n",
      "                x = {field.split()[0]: lambda_fn(ee)\n",
      "                          for field, lambda_fn \n",
      "                          in defaultfields if field.split()[0] in self.classifier.features()}                \n",
      "                if verbose:\n",
      "                    print '\\tComputed features:'\n",
      "                    for k in x: print '\\t\\t', k, x[k]\n",
      "                outcome_predict = self.classifier.predict_instance(x)\n",
      "                # check to move\n",
      "                if self.is_safe_to_slide_(outcome_predict, # recently predict outcome, test to bypass the ss\n",
      "                                              df[self.smags[0]][idx], # v_a\n",
      "                                              df[self.smags[1]][idx], # v_b\n",
      "                                              df[self.smags[2]][idx],\n",
      "                                              verbose = verbose): # v_c\n",
      "                    if verbose: print '\\tUpdating steady state', idx\n",
      "                    self.update_data_window(df, idx)\n",
      "                    updated_ss.append(True)\n",
      "                else:\n",
      "                    if verbose: print '\\tSkipping updating steady state', idx\n",
      "                    updated_ss.append(False)\n",
      "                        \n",
      "                results.append(outcome_predict)\n",
      "                if verbose: print '\\tPrediction outcome:', results[-1]\n",
      "            else:\n",
      "                if verbose: print '\\tSkip since the signal is unvailable'\n",
      "                results.append(nan)\n",
      "                updated_ss.append(False)\n",
      "            v_ss.append((ss_a, ss_b, ss_c)) \n",
      "        return idx, results, v_ss, first_v_ss, updated_ss          \n",
      "    \n",
      "    # return a tuple as\n",
      "    #    self: the object experiment which contains up to date signal data\n",
      "    #    results: the predict outcome \n",
      "    #    start_i: the start index of dataframe in which corespondents to the first predict outcome element in results \n",
      "    def scan_df(self, df, verbose = False):\n",
      "        if self.is2014data:\n",
      "            df = self.convert_2014_data_(df)        \n",
      "        df = df[self.smags + self.sangles + ['DateTime']]\n",
      "        start_idx = df.index.values[0]\n",
      "        classification, steady_states, first_steady_state, updated_steady_state = [], [], {}, []\n",
      "        while(start_idx < df.index[-1]):\n",
      "            idx, results, v_ss, first_v_ss, updated_ss = self.scan_df_(df, start_idx, verbose)\n",
      "            if idx == -1: return None, None, None, None\n",
      "            classification = classification + results\n",
      "            steady_states = steady_states + v_ss\n",
      "            first_steady_state[start_idx] = first_v_ss\n",
      "            updated_steady_state = updated_steady_state + updated_ss\n",
      "            start_idx = idx\n",
      "        return classification, steady_states, first_steady_state, updated_steady_state \n",
      "\n",
      "class ConfigurableClassifyingExperimenter(ClassifyingExperimenter):\n",
      "    def __init__(self, signal, smags, sangles, classifier_pickle, limitted_n_cycle_from_ss):\n",
      "        super(ConfigurableClassifyingExperimenter, self).__init__(signal, smags, sangles, classifier_pickle)\n",
      "        self.limitted_n_cycle_from_ss = limitted_n_cycle_from_ss\n",
      "    \n",
      "    # test the scan should be within datetime of steady state\n",
      "    def is_ss_indatetime(self, dt):\n",
      "        in_out_date = self.last_ss_scan_datetime is not None and \\\n",
      "                0 <= (dt - self.last_ss_scan_datetime).total_seconds() < 0.02 * self.limitted_n_cycle_from_ss\n",
      "        return in_out_date"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting experiment.py\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    }
   ],
   "metadata": {}
  }
 ]
}